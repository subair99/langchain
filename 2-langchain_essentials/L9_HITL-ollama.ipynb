{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c82cb7-58cc-4cf0-86af-5ecefa582a09",
   "metadata": {},
   "source": [
    "# Middleware: Human In The Loop\n",
    "<img src=\"./assets/LC_HITL.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb8ffd-e0ea-41bb-9571-e11890f4f597",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8b742-55cb-49f8-9623-a5ea37c4d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from env_utils import doublecheck_env\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print results\n",
    "doublecheck_env(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693a714-9210-4b57-beb4-13ca063ef067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nest_asyncio\n",
    "from typing import Annotated, List, TypedDict, Literal, Optional\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load database (ensure 'Chinook.db' is in current directory)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# State includes pending tool call for interrupt simulation\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    pending_tool_call: Optional[dict]\n",
    "\n",
    "# Tool: execute SQL\n",
    "@tool(description=\"Execute a read-only SQLite SELECT query.\")\n",
    "def execute_sql(query: str) -> str:\n",
    "    try:\n",
    "        if not query.strip().upper().startswith(\"SELECT\"):\n",
    "            return \"Error: Only SELECT allowed.\"\n",
    "        clean = query.strip().replace('```sql', '').replace('```', '').rstrip(';') + ';'\n",
    "        return str(db.run(clean))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"qwen2.5-coder:7b\", temperature=0.8)\n",
    "\n",
    "# System prompt with real schema\n",
    "SYSTEM_PROMPT = f\"\"\"You are a careful SQL analyst for the Chinook music store.\n",
    "\n",
    "Schema:\n",
    "{db.get_table_info()}\n",
    "\n",
    "Rules:\n",
    "- Use correct table and column names (e.g., Employee.FirstName, not 'name').\n",
    "- Output only one SELECT query when needed.\n",
    "- Do not guess column names.\n",
    "- If you cannot answer, say so clearly.\n",
    "\"\"\"\n",
    "\n",
    "# Node: Call LLM\n",
    "def call_model(state: AgentState):\n",
    "    sys_msg = SystemMessage(content=SYSTEM_PROMPT)\n",
    "    response = llm.invoke([sys_msg] + state[\"messages\"])\n",
    "    \n",
    "    # Fallback: extract SQL if no tool_calls\n",
    "    if not getattr(response, 'tool_calls', None):\n",
    "        import re\n",
    "        match = re.search(r'(SELECT\\s+.*?)(?:;|\\s*$)', response.content, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            response.tool_calls = [{\n",
    "                \"name\": \"execute_sql\",\n",
    "                \"args\": {\"query\": match.group(1).strip() + \";\"},\n",
    "                \"id\": \"manual\"\n",
    "            }]\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Node: Capture pending tool call (for approval)\n",
    "def request_approval(state: AgentState):\n",
    "    last_ai = state[\"messages\"][-1]\n",
    "    tool_call = last_ai.tool_calls[0] if last_ai.tool_calls else None\n",
    "    return {\"pending_tool_call\": tool_call}\n",
    "\n",
    "# Router after LLM\n",
    "def route_after_llm(state: AgentState) -> Literal[\"request_approval\", \"__end__\"]:\n",
    "    last = state[\"messages\"][-1]\n",
    "    if getattr(last, 'tool_calls', None):\n",
    "        return \"request_approval\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"request_approval\", request_approval)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_after_llm,\n",
    "    {\"request_approval\": \"request_approval\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "# Compile app\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Simulated human-in-the-loop runner\n",
    "async def run_with_human_approval(question: str, thread_id: str, approve: bool = True):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    inputs = {\"messages\": [HumanMessage(content=question)]}\n",
    "    \n",
    "    # Run graph until it stops (at request_approval or end)\n",
    "    result = await app.ainvoke(inputs, config)\n",
    "    \n",
    "    last_msg = result[\"messages\"][-1]\n",
    "    \n",
    "    # Check if a tool call was made (needs approval)\n",
    "    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "        tool_call = last_msg.tool_calls[0]\n",
    "        query = tool_call[\"args\"][\"query\"]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ›‘ INTERRUPT: Query requires human approval:\")\n",
    "        print(f\"   {query}\")\n",
    "        print(f\"   Decision: {'APPROVED' if approve else 'REJECTED'}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Simulate human decision\n",
    "        if approve:\n",
    "            tool_output = execute_sql(query)\n",
    "        else:\n",
    "            tool_output = \"Query rejected by human reviewer.\"\n",
    "        \n",
    "        # Inject tool response back into the conversation\n",
    "        tool_message = ToolMessage(\n",
    "            content=tool_output,\n",
    "            tool_call_id=tool_call[\"id\"],\n",
    "            name=tool_call[\"name\"]\n",
    "        )\n",
    "        \n",
    "        # Continue processing with tool result\n",
    "        final_state = await app.ainvoke({\"messages\": [tool_message]}, config)\n",
    "        \n",
    "        # Generate final natural-language answer\n",
    "        sys_msg = SystemMessage(content=SYSTEM_PROMPT)\n",
    "        final_response = llm.invoke([sys_msg] + final_state[\"messages\"])\n",
    "        print(f\"\\n[AI] {final_response.content}\")\n",
    "    \n",
    "    else:\n",
    "        # No tool call needed\n",
    "        print(f\"\\n[AI] {last_msg.content}\")\n",
    "\n",
    "# ðŸ”´ FIRST: Test with REJECTED (False)\n",
    "await run_with_human_approval(\"What are the names of all the employees?\", \"run_reject\", approve=False)\n",
    "\n",
    "# ðŸŸ¢ THEN: Test with APPROVED (True)\n",
    "await run_with_human_approval(\"What are the names of all the employees?\", \"run_approve\", approve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5147c78-83e9-4146-becd-f7e2d8b1a391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
