{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0ebaf1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/trim-filter-messages.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239435-lesson-4-trim-and-filter-messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Filtering and trimming messages\n",
    "\n",
    "## Review\n",
    "\n",
    "Now, we have a deeper understanding of a few things: \n",
    "\n",
    "* How to customize the graph state schema\n",
    "* How to define custom state reducers\n",
    "* How to use multiple graph state schemas\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we can start using these concepts with models in LangGraph!\n",
    " \n",
    "In the next few sessions, we'll build towards a chatbot that has long-term memory.\n",
    "\n",
    "Because our chatbot will use messages, let's first talk a bit more about advanced ways to work with messages in graph state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts).\n",
    "\n",
    "We'll log to a project, `langchain-academy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
   "metadata": {},
   "source": [
    "## Messages as state\n",
    "\n",
    "First, let's define some messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "Yes, I know about whales. But what others should I learn about?\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "messages = [AIMessage(f\"So you said you were researching ocean mammals?\", name=\"Bot\")]\n",
    "messages.append(HumanMessage(f\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
   "metadata": {},
   "source": [
    "Recall we can pass them to a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"granite3.3:8b\", temperature=0)\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
   "metadata": {},
   "source": [
    "We can run our chat model in a simple graph with `MessagesState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "A practical challenge when working with messages is managing long-running conversations. \n",
    "\n",
    "Long-running conversations result in high token usage and latency if we are not careful, because we pass a growing list of messages to the model.\n",
    "\n",
    "We have a few ways to address this.\n",
    "\n",
    "First, recall the trick we saw using `RemoveMessage` and the `add_messages` reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# Nodes\n",
    "def filter_messages(state: MessagesState):\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "def chat_model_node(state: MessagesState):    \n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"filter\", filter_messages)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"filter\")\n",
    "builder.add_edge(\"filter\", \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message list with a preamble\n",
    "messages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\"))\n",
    "messages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\"))\n",
    "messages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))\n",
    "\n",
    "# Invoke\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
   "metadata": {},
   "source": [
    "## Filtering messages\n",
    "\n",
    "If you don't need or want to modify the graph state, you can just filter the messages you pass to the chat model.\n",
    "\n",
    "For example, just pass in a filtered list: `llm.invoke(messages[-1:])` to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
   "metadata": {},
   "source": [
    "Let's take our existing list of messages, append the above LLM response, and append a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me more about Narwhals!\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85563415-c085-46a8-a4ac-155df798c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23349705-a059-47b5-9760-d8f64e687393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke, using message filtering\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
   "metadata": {},
   "source": [
    "The state has all of the mesages.\n",
    "\n",
    "But, let's look at the LangSmith trace to see that the model invocation only uses the last message:\n",
    "\n",
    "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Trim messages\n",
    "\n",
    "Another approach is to [trim messages](https://docs.langchain.com/oss/python/langgraph/add-memory#trim-messages), based upon a set number of tokens. \n",
    "\n",
    "This restricts the message history to a specified number of tokens.\n",
    "\n",
    "While filtering only returns a post-hoc subset of the messages between agents, trimming restricts the number of tokens that a chat model can use to respond.\n",
    "\n",
    "See the `trim_messages` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# Node\n",
    "def chat_model_node(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=llm,\n",
    "            allow_partial=False,\n",
    "        )\n",
    "    return {\"messages\": [llm.invoke(messages)]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c828544-cc25-4a72-9619-223f282892b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of trimming messages\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=100,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm, \n",
    "    allow_partial=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke, using message trimming in the chat_model_node \n",
    "messages_out_trim = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
   "metadata": {},
   "source": [
    "Let's look at the LangSmith trace to see the model invocation:\n",
    "\n",
    "https://smith.langchain.com/public/b153f7e9-f1a5-4d60-8074-f0d7ab5b42ef/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
