{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a93147-c061-457d-b015-537163ee73fe",
   "metadata": {},
   "source": [
    "### Cell 1: Data Models and State Definitions\n",
    "This cell defines the structure of your analysts and the shared memory (State) for the different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722f20fa-1e53-4180-b369-93462fcf86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated, Union\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "\n",
    "# --- Analyst Models ---\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str = Field(description=\"Primary affiliation of the analyst.\")\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "    role: str = Field(description=\"Role of the analyst in the context of the topic.\")\n",
    "    description: str = Field(description=\"Description of the analyst focus, concerns, and motives.\")\n",
    "    \n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst] = Field(description=\"Comprehensive list of analysts with their roles and affiliations.\")\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")\n",
    "\n",
    "# --- Graph States ---\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: str \n",
    "    max_analysts: int \n",
    "    human_analyst_feedback: str \n",
    "    analysts: List[Analyst] \n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[Union[AIMessage, HumanMessage, SystemMessage]], operator.add]\n",
    "    max_num_turns: int \n",
    "    context: Annotated[list, operator.add] \n",
    "    analyst: Analyst \n",
    "    interview: str \n",
    "    sections: list \n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str \n",
    "    max_analysts: int \n",
    "    human_analyst_feedback: str \n",
    "    analysts: List[Analyst] \n",
    "    sections: Annotated[list, operator.add] \n",
    "    introduction: str \n",
    "    content: str \n",
    "    conclusion: str \n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbea1ed-24c4-47d4-b746-662c14527205",
   "metadata": {},
   "source": [
    "### Cell 2: Personas Generation Logic\n",
    "This cell handles creating the AI analysts and managing human feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34a0b9b-7aba-49c8-926c-f37fc8df9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "1. Review the research topic: {topic}\n",
    "2. Examine editorial feedback: {human_analyst_feedback}\n",
    "3. Determine interesting themes and pick the top {max_analysts} themes.\n",
    "4. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
    "        \n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "    system_message = analyst_instructions.format(topic=topic,\n",
    "                                                 human_analyst_feedback=human_analyst_feedback, \n",
    "                                                 max_analysts=max_analysts)\n",
    "\n",
    "    res = structured_llm.invoke([SystemMessage(content=system_message)] + \n",
    "                                [HumanMessage(content=\"Generate the set of analysts.\")])\n",
    "    return {\"analysts\": res.analysts}\n",
    "\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\" No-op node for interruption \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d5a8d-c43c-4b82-a838-d00217d6c805",
   "metadata": {},
   "source": [
    "### Cell 3: The Interview Sub-Graph Nodes\n",
    "This cell contains the specialized nodes for searching the web, Wikipedia, and conducting the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f7dc71-4d45-4d7f-a5eb-3084e9b98e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"output_schema\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n",
      "/home/abdul/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langchain_tavily/tavily_research.py:97: UserWarning: Field name \"stream\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  class TavilyResearch(BaseTool):  # type: ignore[override, override]\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    res = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "    query = getattr(res, 'search_query', None)\n",
    "    \n",
    "    if not query: return {\"context\": []}\n",
    "\n",
    "    data = tavily_search.invoke({\"query\": query})\n",
    "    search_docs = data.get(\"results\", data) if isinstance(data, dict) else data\n",
    "    \n",
    "    formatted = \"\\n\\n---\\n\\n\".join([f'<Document href=\"{d.get(\"url\",\"\")}\"/>\\n{d.get(\"content\",\"\")}\\n</Document>' for d in search_docs])\n",
    "    return {\"context\": [formatted]} \n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    res = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "    query = getattr(res, 'search_query', None)\n",
    "\n",
    "    if not query: return {\"context\": []}\n",
    "\n",
    "    try:\n",
    "        docs = WikipediaLoader(query=query, load_max_docs=2).load()\n",
    "        formatted = \"\\n\\n---\\n\\n\".join([f'<Document source=\"{d.metadata.get(\"source\",\"\")}\" page=\"{d.metadata.get(\"page\", \"\")}\"/>\\n{d.page_content}\\n</Document>' for d in docs])\n",
    "        return {\"context\": [formatted]}\n",
    "    except:\n",
    "        return {\"context\": []}\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    return {\"messages\": [question]}\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=state[\"context\"])\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    answer.name = \"expert\"\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    return {\"interview\": get_buffer_string(state[\"messages\"])}\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and getattr(m, 'name', '') == name])\n",
    "    if num_responses >= state.get('max_num_turns', 2) or \"Thank you so much for your help\" in messages[-2].content:\n",
    "        return 'save_interview'\n",
    "    return \"ask_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c53c7-364f-4274-9ec9-5d73a8c68983",
   "metadata": {},
   "source": [
    "### Cell 4: Writing and Finalizing the Report\n",
    "This cell handles the \"Reduce\" phase, where individual analyst memos are turned into a coherent document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d366a8-de79-4939-814c-8e6e8b2879d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_section(state: InterviewState):\n",
    "    system_message = section_writer_instructions.format(focus=state[\"analyst\"].description)\n",
    "    section = llm.invoke([SystemMessage(content=system_message)] + \n",
    "                         [HumanMessage(content=f\"Use this source: {state['context']}\")])\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    context = \"\\n\\n\".join([f\"{s}\" for s in state[\"sections\"]])\n",
    "    system_message = report_writer_instructions.format(topic=state[\"topic\"], context=context)\n",
    "    report = llm.invoke([SystemMessage(content=system_message)] + [HumanMessage(content=\"Write report.\")])\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    context = \"\\n\\n\".join([f\"{s}\" for s in state[\"sections\"]])\n",
    "    instructions = intro_conclusion_instructions.format(topic=state[\"topic\"], formatted_str_sections=context)\n",
    "    intro = llm.invoke([instructions] + [HumanMessage(content=\"Write introduction\")])\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    context = \"\\n\\n\".join([f\"{s}\" for s in state[\"sections\"]])\n",
    "    instructions = intro_conclusion_instructions.format(topic=state[\"topic\"], formatted_str_sections=context)\n",
    "    conclusion = llm.invoke([instructions] + [HumanMessage(content=\"Write conclusion\")])\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    content = state[\"content\"].replace(\"## Insights\", \"\").strip()\n",
    "    sources = content.split(\"\\n## Sources\\n\")[1] if \"## Sources\" in content else None\n",
    "    content = content.split(\"\\n## Sources\\n\")[0] if sources else content\n",
    "    \n",
    "    final_report = f\"{state['introduction']}\\n\\n---\\n\\n{content}\\n\\n---\\n\\n{state['conclusion']}\"\n",
    "    if sources: final_report += f\"\\n\\n## Sources\\n{sources}\"\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bfa8f-4c8c-4f6b-826a-2e08e92dc0e3",
   "metadata": {},
   "source": [
    "### Cell 5: Main Orchestration and Graph Compilation\n",
    "This cell ties the sub-graph and the main graph together and sets up the Parallel Send logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c982dd15-eb1e-4559-9728-37dd03e9abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    if state.get('human_analyst_feedback'):\n",
    "        return \"create_analysts\"\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    return [Send(\"conduct_interview\", {\n",
    "        \"analyst\": a,\n",
    "        \"messages\": [HumanMessage(content=f\"So you said you were writing an article on {topic}?\")],\n",
    "        \"max_num_turns\": 2\n",
    "    }) for a in state[\"analysts\"]]\n",
    "\n",
    "# --- Compile Sub-Graph ---\n",
    "it_builder = StateGraph(InterviewState)\n",
    "it_builder.add_node(\"ask_question\", generate_question)\n",
    "it_builder.add_node(\"search_web\", search_web); it_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "it_builder.add_node(\"answer_question\", generate_answer)\n",
    "it_builder.add_node(\"save_interview\", save_interview); it_builder.add_node(\"write_section\", write_section)\n",
    "it_builder.add_edge(START, \"ask_question\")\n",
    "it_builder.add_edge(\"ask_question\", \"search_web\"); it_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
    "it_builder.add_edge(\"search_web\", \"answer_question\"); it_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
    "it_builder.add_conditional_edges(\"answer_question\", route_messages, ['ask_question', 'save_interview'])\n",
    "it_builder.add_edge(\"save_interview\", \"write_section\"); it_builder.add_edge(\"write_section\", END)\n",
    "interview_graph = it_builder.compile()\n",
    "\n",
    "# --- Compile Main Graph ---\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_graph)\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e840-ad37-48fc-85e1-95764ccda808",
   "metadata": {},
   "source": [
    "### Cell 6: Instructions & Prompts for Research Orchestration\n",
    "This cell acts as the \"Instruction Manual\" or \"Brain Configuration\" for the entire graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aff3deb-fe68-4343-8fb4-7d437284a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Analyst Question Generation (used in generate_question) ---\n",
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "\n",
    "# --- 2. Search Query Generation (used in search_web & search_wikipedia) ---\n",
    "search_instructions = SystemMessage(content=\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "        \n",
    "First, analyze the full conversation. Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\")\n",
    "\n",
    "\n",
    "# --- 3. Expert Answer Generation (used in generate_answer) ---\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "Guidelines:\n",
    "1. Use only the information provided in the context. \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "4. Include these sources your answer next to any relevant statements (e.g., [1]).\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\"\"\"\n",
    "\n",
    "\n",
    "# --- 4. Section Writing (used in write_section) ---\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
    "\n",
    "1. Create a report structure using markdown formatting (## for Title, ### for headers).\n",
    "2. Make your title engaging based upon the focus area: {focus}\n",
    "3. Emphasize what is novel, interesting, or surprising gathered from the interview.\n",
    "4. Use numbered sources in your report (e.g., [1], [2]).\n",
    "5. Aim for approximately 400 words maximum.\"\"\"\n",
    "\n",
    "\n",
    "# --- 5. Final Report Synthesis (used in write_report) ---\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic: {topic}\n",
    "    \n",
    "Consolidate memos from your analysts into a cohesive narrative.\n",
    "1. Use markdown formatting. \n",
    "2. Include no pre-amble.\n",
    "3. Start with title header: ## Insights\n",
    "4. Preserve citations in the memos (e.g., [1]).\n",
    "5. Create a final, consolidated ## Sources section at the end.\"\"\"\n",
    "\n",
    "\n",
    "# --- 6. Intro & Conclusion Writing (used in write_introduction & write_conclusion) ---\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "Write a crisp (~100 words) and compelling introduction or conclusion as instructed.\n",
    "Include no pre-amble.\n",
    "For Intro: Use # for title and ## Introduction header.\n",
    "For Conclusion: Use ## Conclusion header.\n",
    "\n",
    "Sections to reflect on: {formatted_str_sections}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb51cc-627e-4017-a19b-76e63fabc9bb",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5532960-3284-4f87-a4ce-d655487d8c5f",
   "metadata": {},
   "source": [
    "### Cell 1: Initial Run (Persona Generation)\n",
    "This starts the process, generates the analysts, and then stops at the human_feedback node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2ea60b-50d8-49f7-a10b-b6e337a60bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Persona Generation ---\n",
      "\n",
      "Generated 3 Analysts:\n",
      "- Dr. Rachel Kim (AI Researcher): Research Institute\n",
      "- Alex Chen (Software Engineer): Tech Firm\n",
      "- Dr. Liam Patel (Computer Science Professor): Academic Institution\n",
      "\n",
      "Next node to execute: ('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# Setup the thread and initial inputs\n",
    "thread = {\"configurable\": {\"thread_id\": \"simulation_1\"}}\n",
    "initial_input = {\n",
    "    \"topic\": \"The benefits of adopting LangGraph as an agent framework\",\n",
    "    \"max_analysts\": 3\n",
    "}\n",
    "\n",
    "print(\"--- Starting Persona Generation ---\")\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', [])\n",
    "    if analysts:\n",
    "        print(f\"\\nGenerated {len(analysts)} Analysts:\")\n",
    "        for a in analysts:\n",
    "            print(f\"- {a.name} ({a.role}): {a.affiliation}\")\n",
    "\n",
    "# Verification: The graph should now be 'hibernating'\n",
    "state = graph.get_state(thread)\n",
    "print(f\"\\nNext node to execute: {state.next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7e79b-79c6-4205-a862-0b5e3f9b6b91",
   "metadata": {},
   "source": [
    "### Cell 2: Providing Feedback (The Loop)\n",
    "If you aren't happy with the analysts, run this cell to tell the AI what to change. This triggers the conditional_edge to go back to create_analysts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb01f15-aebd-4ba8-b89d-e5770e36137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feedback Sent. Regenerating Analysts... ---\n",
      "Updated Name: Dr. Rachel Kim | Role: AI Researcher\n",
      "Updated Name: Alex Chen | Role: Software Engineer\n",
      "Updated Name: Dr. Liam Patel | Role: Computer Science Professor\n",
      "Updated Name: Dr. Rachel Kim | Role: AI Research Analyst\n",
      "Updated Name: Dr. Eric Thompson | Role: AI Research Analyst\n",
      "Updated Name: Dr. Maria Rodriguez | Role: AI Research Analyst\n",
      "Updated Name: Dr. David Lee | Role: AI Research Analyst\n",
      "Updated Name: Dr. Sophia Patel | Role: AI Research Analyst\n"
     ]
    }
   ],
   "source": [
    "# Provide feedback to the graph\n",
    "feedback = \"Add a skeptical researcher who is worried about the complexity of state management.\"\n",
    "\n",
    "graph.update_state(\n",
    "    thread, \n",
    "    {\"human_analyst_feedback\": feedback}, \n",
    "    as_node=\"human_feedback\"\n",
    ")\n",
    "\n",
    "print(\"--- Feedback Sent. Regenerating Analysts... ---\")\n",
    "# Resume execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', [])\n",
    "    if analysts:\n",
    "        for a in analysts:\n",
    "            print(f\"Updated Name: {a.name} | Role: {a.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b1ee2-6dd7-487b-b48d-25556d6129d1",
   "metadata": {},
   "source": [
    "### Cell 3: Approving and Executing Parallel Interviews\n",
    "Once you are happy, you must clear the feedback to satisfy the initiate_all_interviews logic. This will kick off the parallel Send API calls to the interview sub-graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80554d48-b85b-43ad-9f84-7d7ba1600285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Approval Received. Starting Parallel Interviews & Report Writing... ---\n",
      "(This may take a minute as multiple analysts are researching simultaneously)\n"
     ]
    }
   ],
   "source": [
    "# Signal approval by setting feedback to None/Empty\n",
    "graph.update_state(\n",
    "    thread, \n",
    "    {\"human_analyst_feedback\": None}, \n",
    "    as_node=\"human_feedback\"\n",
    ")\n",
    "\n",
    "print(\"--- Approval Received. Starting Parallel Interviews & Report Writing... ---\")\n",
    "print(\"(This may take a minute as multiple analysts are researching simultaneously)\")\n",
    "\n",
    "# We use stream_mode=\"updates\" here so you can see the nodes finishing in real-time\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    for node_name, data in event.items():\n",
    "        print(f\"Finished Node: {node_name}\")\n",
    "        \n",
    "print(\"\\n--- Research Complete! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b895f-ee68-4df0-ac74-21298cc6f4ca",
   "metadata": {},
   "source": [
    "### Cell 4: Viewing the Final Report\n",
    "After the parallel work is done, the \"Reduce\" nodes (write_report, write_introduction, etc.) will have finished. Run this to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b8e96-01c4-4bbf-a838-5673fe46c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Get the final state from the thread\n",
    "final_state = graph.get_state(thread)\n",
    "report_content = final_state.values.get('final_report')\n",
    "\n",
    "if report_content:\n",
    "    display(Markdown(report_content))\n",
    "else:\n",
    "    print(\"Report not found. Ensure the graph finished the finalize_report node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {},
   "source": [
    "We can look at the trace:\n",
    "\n",
    "https://smith.langchain.com/public/2933a7bb-bcef-4d2d-9b85-cc735b22ca0c/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bd094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
