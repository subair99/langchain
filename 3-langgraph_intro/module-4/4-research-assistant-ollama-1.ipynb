{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61652-903a-45cf-b95b-1bfb77d774ec",
   "metadata": {},
   "source": [
    "## Core Logic and Graph Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0f7fd-796b-4fdc-ae75-7e708a9544d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "from typing import List, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0, format=\"json\")\n",
    "search_tool = TavilySearch(max_results=2)\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str; name: str; role: str; description: str\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    analyst: Analyst \n",
    "    context: Annotated[list, operator.add]\n",
    "    sections: list\n",
    "\n",
    "# --- 2. ROBUST NODES ---\n",
    "def ask_question(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    sys_msg = f\"You are {analyst.name}, {analyst.role}. Ask a targeted research question.\"\n",
    "    return {\"messages\": [llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])]}\n",
    "\n",
    "def search_node(state: InterviewState):\n",
    "    sys_msg = SystemMessage(content=\"Output ONLY a JSON object with a 'query' key.\")\n",
    "    try:\n",
    "        # Attempt structured output\n",
    "        structured_llm = llm.with_structured_output(SearchQuery)\n",
    "        res = structured_llm.invoke([sys_msg] + state[\"messages\"])\n",
    "        query_text = res.query\n",
    "    except Exception:\n",
    "        # Manual Fallback if JSON is messy\n",
    "        res = llm.invoke([sys_msg] + state[\"messages\"])\n",
    "        clean = res.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        try:\n",
    "            query_text = json.loads(clean)[\"query\"]\n",
    "        except:\n",
    "            query_text = \"LangGraph agentic patterns\" # Last resort\n",
    "    \n",
    "    search_data = search_tool.invoke(query_text)\n",
    "    content = \"\\n\".join([f\"Source: {r['url']}\\nContent: {r['content']}\" for r in search_data.get('results', [])])\n",
    "    return {\"context\": [content]}\n",
    "\n",
    "def answer_node(state: InterviewState):\n",
    "    context_str = \"\\n\".join(state[\"context\"])\n",
    "    sys_msg = f\"Answer using ONLY this context: {context_str}. Be concise.\"\n",
    "    res = llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])\n",
    "    return {\"messages\": [res], \"sections\": [res.content]}\n",
    "\n",
    "# --- 3. BUILD SUB-GRAPH ---\n",
    "itv_builder = StateGraph(InterviewState)\n",
    "itv_builder.add_node(\"ask\", ask_question); itv_builder.add_node(\"search\", search_node); itv_builder.add_node(\"answer\", answer_node)\n",
    "itv_builder.add_edge(START, \"ask\"); itv_builder.add_edge(\"ask\", \"search\"); itv_builder.add_edge(\"search\", \"answer\"); itv_builder.add_edge(\"answer\", END)\n",
    "interview_graph = itv_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce4fd-e258-4617-b3a0-dda8ab2632f1",
   "metadata": {},
   "source": [
    "## Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a6c77a-f518-4fed-bfa8-c5f9d87b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str; max_analysts: int; human_analyst_feedback: str\n",
    "    analysts: List[Analyst]; sections: Annotated[list, operator.add]; final_report: str\n",
    "\n",
    "def create_analysts(state: ResearchState):\n",
    "    prompt = f\"Create a team of {state['max_analysts']} analysts for: {state['topic']}. Return JSON.\"\n",
    "    try:\n",
    "        res = llm.with_structured_output(Perspectives).invoke([SystemMessage(content=prompt)])\n",
    "        return {\"analysts\": res.analysts}\n",
    "    except Exception:\n",
    "        # Fallback for analyst creation\n",
    "        return {\"analysts\": [Analyst(name=\"Lead Researcher\", role=\"Expert\", affiliation=\"Local\", description=\"Generalist\")]}\n",
    "\n",
    "def initiate_interviews(state: ResearchState):\n",
    "    return [Send(\"conduct_interview\", {\"analyst\": a, \"messages\": [HumanMessage(content=f\"Topic: {state['topic']}\")]}) for a in state[\"analysts\"]]\n",
    "\n",
    "def compile_report(state: ResearchState):\n",
    "    all_sections = \"\\n\\n\".join(state[\"sections\"])\n",
    "    res = llm.invoke(f\"Write a Markdown report based on these interviews:\\n\\n{all_sections}\")\n",
    "    return {\"final_report\": res.content}\n",
    "\n",
    "builder = StateGraph(ResearchState)\n",
    "builder.add_node(\"create_analysts\", create_analysts); builder.add_node(\"human_feedback\", lambda s: None)\n",
    "builder.add_node(\"conduct_interview\", interview_graph); builder.add_node(\"write_report\", compile_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\"); builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", \n",
    "    lambda s: initiate_interviews(s) if s.get(\"human_analyst_feedback\") == \"OK\" else \"create_analysts\",\n",
    "    {\"conduct_interview\": \"conduct_interview\", \"create_analysts\": \"create_analysts\"})\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\"); builder.add_edge(\"write_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277fa65-0378-4d3e-9107-f61407fd454e",
   "metadata": {},
   "source": [
    "## Provide Approval and Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923a609d-ce3b-436c-8323-a654da6fbe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysts Created: ['Dr. Elena Martinez', 'Dr. Raj Patel', 'Dr. Sarah Kim']\n",
      "\n",
      "Processing interviews one by one (Safer for CPU)...\n",
      "-- Completed Node: conduct_interview --\n",
      "-- Completed Node: conduct_interview --\n",
      "-- Completed Node: conduct_interview --\n",
      "-- Completed Node: write_report --\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"report\": {\n",
       "    \"title\": \"Optimizing Parallelization of Local LLM Workflows with LangGraph\",\n",
       "    \"introduction\": \"LangGraph is a powerful framework for orchestrating and managing workflows in large language models (LLMs). As the demand for efficient and scalable LLM inference grows, optimizing parallelization becomes critical. This report explores how LangGraph can be leveraged to enhance the parallelization of local LLM workflows, addressing key factors such as task dependencies, resource allocation, and performance trade-offs. The analysis is based on insights from interviews with experts in the field.\",\n",
       "    \"sections\": [\n",
       "      {\n",
       "        \"title\": \"1. Leveraging LangGraph for Parallelization of Local LLM Workflows\",\n",
       "        \"content\": \"LangGraph's ability to model task dependencies as directed acyclic graphs (DAGs) enables efficient parallelization of LLM workflows. By explicitly defining dependencies between tasks, LangGraph ensures that tasks are executed in the correct order while maximizing concurrency. Key considerations include:\\n\\n- **Task Dependency Management**: LangGraph's stateful execution model allows for dynamic dependency resolution, ensuring that tasks are only executed when their prerequisites are met.\\n- **Resource Allocation**: The framework supports dynamic resource allocation, enabling the system to scale compute resources (e.g., CPU/GPU) based on workload demands. This reduces idle time and optimizes throughput.\\n- **Performance Trade-offs**: While parallelization improves throughput, it may introduce latency due to task scheduling overhead. LangGraph's prioritization of critical paths and caching mechanisms helps mitigate these trade-offs.\\n\\n**Recommendation**: Use LangGraph's DAG-based workflow management to balance task dependencies and resource allocation, while implementing caching and prioritization strategies to minimize latency.\"\n",
       "      },\n",
       "      {\n",
       "        \"\":\"\",\n",
       "        \"content\": \"Optimizing the parallelization of local LLM inference tasks requires careful consideration of task granularity, resource contention, and throughput. LangGraph's modular architecture allows for fine-grained task decomposition, enabling parallel execution of independent subtasks. Key trade-offs include:\\n\\n- **Task Granularity**: Smaller tasks improve parallelism but may increase overhead due to task scheduling. LangGraph's ability to dynamically adjust granularity based on workload ensures optimal performance.\\n- **Resource Contention**: Parallel execution can lead to resource contention (e.g., GPU memory, CPU cores). LangGraph's resource-aware scheduling and load balancing mechanisms help mitigate this by prioritizing high-priority tasks and distributing workloads evenly.\\n- **Throughput Optimization**: Batch processing and pipelining techniques can be integrated into LangGraph workflows to maximize throughput while maintaining low latency.\\n\\n**Recommendation**: Implement dynamic task granularity and resource-aware scheduling in LangGraph to balance parallelism and resource utilization, while leveraging batch processing for throughput optimization.\"\n",
       "      },\n",
       "      {\n",
       "        \"title\": \"3. Optimizing LangGraph's Architecture for Parallelization Efficiency\",\n",
       "        \"content\": \"To enhance the parallelization efficiency of local LLMs, LangGraph's architecture must be optimized for scalability, latency, and model accuracy. Key strategies include:\\n\\n- **Modular Architecture**: LangGraph's modular design allows for the integration of specialized components (e.g., model quantization, caching layers) that improve performance without sacrificing accuracy.\\n- **Latency-Accuracy Trade-offs**: Techniques such as model pruning, quantization, and caching can reduce inference latency while maintaining acceptable accuracy. LangGraph's support for hybrid execution modes enables these optimizations.\\n- **Distributed Execution**: For large-scale workflows, LangGraph can be extended to support distributed execution across multiple nodes, balancing computational load and minimizing bottlenecks.\\n\\n**Recommendation**: Optimize LangGraph's architecture by integrating model-specific optimizations (e.g., quantization) and extending it for distributed execution to balance latency, accuracy, and resource utilization.\"\n",
       "      }\n",
       "    ],\n",
       "    \"conclusion\": \"LangGraph offers a robust foundation for optimizing the parallelization of local LLM workflows. By addressing task dependencies, resource allocation, and performance trade-offs, LangGraph can significantly enhance throughput and reduce latency. Future work should focus on extending LangGraph's capabilities for distributed execution and integrating advanced model optimization techniques to further improve efficiency.\",\n",
       "    \"references\": [\n",
       "      \"LangGraph Documentation\",\n",
       "      \"LLM Parallelization Best Practices\",\n",
       "      \"Workflow Management Systems for AI\"\n",
       "    ]\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unique ID to ensure we don't reload old broken states\n",
    "config = {\"configurable\": {\"thread_id\": \"Harris_Final_Stable_v1\"}, \"recursion_limit\": 50}\n",
    "\n",
    "# 1. Start\n",
    "initial_input = {\"topic\": \"Local LLM Parallelization with LangGraph\", \"max_analysts\": 3}\n",
    "for event in graph.stream(initial_input, config, stream_mode=\"values\"):\n",
    "    if \"analysts\" in event:\n",
    "        print(f\"Analysts Created: {[a.name for a in event['analysts']]}\")\n",
    "\n",
    "# 2. Update status for the gatekeeper\n",
    "graph.update_state(config, {\"human_analyst_feedback\": \"OK\"}, as_node=\"human_feedback\")\n",
    "\n",
    "# 3. Resume (The Map-Reduce step)\n",
    "print(\"\\nProcessing interviews one by one (Safer for CPU)...\")\n",
    "for event in graph.stream(None, config, stream_mode=\"updates\", max_concurrency=1):\n",
    "    node_name = list(event.keys())[0]\n",
    "    print(f\"-- Completed Node: {node_name} --\")\n",
    "\n",
    "# 4. Show final output\n",
    "final_state = graph.get_state(config)\n",
    "if 'final_report' in final_state.values:\n",
    "    display(Markdown(final_state.values['final_report']))\n",
    "else:\n",
    "    print(\"Report generation in progress or failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d4cf4-a205-4c19-b605-eb8a4b0860ad",
   "metadata": {},
   "source": [
    "## Professional Formatted Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0abdca68-c111-43b8-a357-02bb0d300fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Optimizing Parallelization of Local LLM Workflows with LangGraph\n",
       "\n",
       "## Introduction\n",
       "LangGraph is a powerful framework for orchestrating and managing workflows in large language models (LLMs). As the demand for efficient and scalable LLM inference grows, optimizing parallelization becomes critical. This report explores how LangGraph can be leveraged to enhance the parallelization of local LLM workflows, addressing key factors such as task dependencies, resource allocation, and performance trade-offs. The analysis is based on insights from interviews with experts in the field.\n",
       "\n",
       "### 1. Leveraging LangGraph for Parallelization of Local LLM Workflows\n",
       "LangGraph's ability to model task dependencies as directed acyclic graphs (DAGs) enables efficient parallelization of LLM workflows. By explicitly defining dependencies between tasks, LangGraph ensures that tasks are executed in the correct order while maximizing concurrency. Key considerations include:\n",
       "\n",
       "- **Task Dependency Management**: LangGraph's stateful execution model allows for dynamic dependency resolution, ensuring that tasks are only executed when their prerequisites are met.\n",
       "- **Resource Allocation**: The framework supports dynamic resource allocation, enabling the system to scale compute resources (e.g., CPU/GPU) based on workload demands. This reduces idle time and optimizes throughput.\n",
       "- **Performance Trade-offs**: While parallelization improves throughput, it may introduce latency due to task scheduling overhead. LangGraph's prioritization of critical paths and caching mechanisms helps mitigate these trade-offs.\n",
       "\n",
       "**Recommendation**: Use LangGraph's DAG-based workflow management to balance task dependencies and resource allocation, while implementing caching and prioritization strategies to minimize latency.\n",
       "\n",
       "### Analysis\n",
       "Optimizing the parallelization of local LLM inference tasks requires careful consideration of task granularity, resource contention, and throughput. LangGraph's modular architecture allows for fine-grained task decomposition, enabling parallel execution of independent subtasks. Key trade-offs include:\n",
       "\n",
       "- **Task Granularity**: Smaller tasks improve parallelism but may increase overhead due to task scheduling. LangGraph's ability to dynamically adjust granularity based on workload ensures optimal performance.\n",
       "- **Resource Contention**: Parallel execution can lead to resource contention (e.g., GPU memory, CPU cores). LangGraph's resource-aware scheduling and load balancing mechanisms help mitigate this by prioritizing high-priority tasks and distributing workloads evenly.\n",
       "- **Throughput Optimization**: Batch processing and pipelining techniques can be integrated into LangGraph workflows to maximize throughput while maintaining low latency.\n",
       "\n",
       "**Recommendation**: Implement dynamic task granularity and resource-aware scheduling in LangGraph to balance parallelism and resource utilization, while leveraging batch processing for throughput optimization.\n",
       "\n",
       "### 3. Optimizing LangGraph's Architecture for Parallelization Efficiency\n",
       "To enhance the parallelization efficiency of local LLMs, LangGraph's architecture must be optimized for scalability, latency, and model accuracy. Key strategies include:\n",
       "\n",
       "- **Modular Architecture**: LangGraph's modular design allows for the integration of specialized components (e.g., model quantization, caching layers) that improve performance without sacrificing accuracy.\n",
       "- **Latency-Accuracy Trade-offs**: Techniques such as model pruning, quantization, and caching can reduce inference latency while maintaining acceptable accuracy. LangGraph's support for hybrid execution modes enables these optimizations.\n",
       "- **Distributed Execution**: For large-scale workflows, LangGraph can be extended to support distributed execution across multiple nodes, balancing computational load and minimizing bottlenecks.\n",
       "\n",
       "**Recommendation**: Optimize LangGraph's architecture by integrating model-specific optimizations (e.g., quantization) and extending it for distributed execution to balance latency, accuracy, and resource utilization.\n",
       "\n",
       "## Conclusion\n",
       "LangGraph offers a robust foundation for optimizing the parallelization of local LLM workflows. By addressing task dependencies, resource allocation, and performance trade-offs, LangGraph can significantly enhance throughput and reduce latency. Future work should focus on extending LangGraph's capabilities for distributed execution and integrating advanced model optimization techniques to further improve efficiency.\n",
       "\n",
       "## References\n",
       "* LangGraph Documentation\n",
       "* LLM Parallelization Best Practices\n",
       "* Workflow Management Systems for AI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Extract the report from the final state of the graph\n",
    "final_state = graph.get_state(config)\n",
    "report_raw = final_state.values.get('final_report', '')\n",
    "\n",
    "# 2. Try to parse and format the JSON report\n",
    "try:\n",
    "    # Handle the case where the LLM wrapped it in Markdown code blocks\n",
    "    clean_json = report_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    data = json.loads(clean_json)\n",
    "    \n",
    "    # Check if 'report' is the top-level key (as seen in your output)\n",
    "    content = data.get('report', data)\n",
    "    \n",
    "    markdown_out = f\"# {content.get('title', 'Research Report')}\\n\\n\"\n",
    "    markdown_out += f\"## Introduction\\n{content.get('introduction', '')}\\n\\n\"\n",
    "    \n",
    "    for section in content.get('sections', []):\n",
    "        title = section.get('title') or \"Analysis\"\n",
    "        markdown_out += f\"### {title}\\n{section.get('content', '')}\\n\\n\"\n",
    "        \n",
    "    markdown_out += f\"## Conclusion\\n{content.get('conclusion', '')}\\n\\n\"\n",
    "    \n",
    "    if 'references' in content:\n",
    "        markdown_out += \"## References\\n* \" + \"\\n* \".join(content['references'])\n",
    "    \n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback: Just print the raw text if JSON parsing fails\n",
    "    print(\"Parsing failed or report already in Markdown. Displaying raw output:\")\n",
    "    display(Markdown(report_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824da862-a1b3-4ffb-b33c-da724d6b7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
