{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61652-903a-45cf-b95b-1bfb77d774ec",
   "metadata": {},
   "source": [
    "## Core Logic and Graph Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0f7fd-796b-4fdc-ae75-7e708a9544d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "from typing import List, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "# Note: Ensure your TAVILY_API_KEY is set in your environment variables\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0)\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str; name: str; role: str; description: str\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    analyst: Analyst \n",
    "    context: Annotated[list, operator.add]\n",
    "    sections: list\n",
    "    sources: Annotated[list, operator.add] # New: Track URLs\n",
    "\n",
    "# --- 2. NODES ---\n",
    "def ask_question(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    sys_msg = f\"You are {analyst.name}, {analyst.role}. Ask a technical question about the topic.\"\n",
    "    return {\"messages\": [llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])]}\n",
    "\n",
    "def search_node(state: InterviewState):\n",
    "    \"\"\"Generates a search query and fetches live data from Tavily.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1].content\n",
    "    sys_msg = SystemMessage(content=\"Output ONLY a JSON object with a 'query' key for web search.\")\n",
    "    \n",
    "    # Generate query\n",
    "    res = llm.with_structured_output(SearchQuery).invoke([sys_msg, HumanMessage(content=last_msg)])\n",
    "    print(f\"üîç {state['analyst'].name} is researching: {res.query}\")\n",
    "    \n",
    "    # Execute search\n",
    "    search_data = search_tool.invoke(res.query)\n",
    "    \n",
    "    content_list = []\n",
    "    source_links = []\n",
    "    for r in search_data:\n",
    "        content_list.append(f\"Source: {r['url']}\\nContent: {r['content']}\")\n",
    "        source_links.append(r['url'])\n",
    "        \n",
    "    return {\n",
    "        \"context\": [\"\\n\\n\".join(content_list)], \n",
    "        \"sources\": source_links\n",
    "    }\n",
    "\n",
    "def answer_node(state: InterviewState):\n",
    "    context_str = \"\\n\".join(state[\"context\"])\n",
    "    sys_msg = f\"Using this research:\\n{context_str}\\n\\nAnswer the analyst's question. Be technical.\"\n",
    "    res = llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])\n",
    "    return {\"messages\": [res], \"sections\": [res.content]}\n",
    "\n",
    "# --- 3. SUB-GRAPH BUILD ---\n",
    "itv_builder = StateGraph(InterviewState)\n",
    "itv_builder.add_node(\"ask\", ask_question)\n",
    "itv_builder.add_node(\"search\", search_node)\n",
    "itv_builder.add_node(\"answer\", answer_node)\n",
    "itv_builder.add_edge(START, \"ask\")\n",
    "itv_builder.add_edge(\"ask\", \"search\")\n",
    "itv_builder.add_edge(\"search\", \"answer\")\n",
    "itv_builder.add_edge(\"answer\", END)\n",
    "interview_graph = itv_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce4fd-e258-4617-b3a0-dda8ab2632f1",
   "metadata": {},
   "source": [
    "## Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a6c77a-f518-4fed-bfa8-c5f9d87b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str; max_analysts: int; human_analyst_feedback: str\n",
    "    analysts: List[Analyst]; sections: Annotated[list, operator.add]\n",
    "    sources: Annotated[list, operator.add]; final_report: str\n",
    "\n",
    "def create_analysts(state: ResearchState):\n",
    "    prompt = f\"Create a team of {state['max_analysts']} analysts for: {state['topic']}. Return JSON.\"\n",
    "    res = llm.with_structured_output(Perspectives).invoke([SystemMessage(content=prompt)])\n",
    "    \n",
    "    # PRINTING IN YOUR REQUESTED FORMAT\n",
    "    print(\"\\n--- ANALYST TEAM GENERATED ---\")\n",
    "    for a in res.analysts:\n",
    "        print(f\"Name: {a.name}\")\n",
    "        print(f\"Affiliation: {a.affiliation}\")\n",
    "        print(f\"Role: {a.role}\")\n",
    "        print(f\"Description: {a.description}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return {\"analysts\": res.analysts}\n",
    "\n",
    "def initiate_interviews(state: ResearchState):\n",
    "    return [Send(\"conduct_interview\", {\"analyst\": a, \"messages\": [HumanMessage(content=state[\"topic\"])]}) for a in state[\"analysts\"]]\n",
    "\n",
    "def compile_report(state: ResearchState):\n",
    "    all_sections = \"\\n\\n\".join(state[\"sections\"])\n",
    "    # Format sources as a clean Markdown list\n",
    "    unique_sources = list(set(state.get(\"sources\", [])))\n",
    "    source_str = \"\\n\".join([f\"* [{s}]({s})\" for s in unique_sources])\n",
    "    \n",
    "    prompt = f\"Write a professional report using these sections:\\n{all_sections}\\n\\nAt the very end, add a section called '### References' and list these exact links: {source_str}\"\n",
    "    res = llm.invoke(prompt)\n",
    "    return {\"final_report\": res.content}\n",
    "\n",
    "# GRAPH BUILDER\n",
    "builder = StateGraph(ResearchState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", lambda s: None)\n",
    "builder.add_node(\"conduct_interview\", interview_graph)\n",
    "builder.add_node(\"write_report\", compile_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", \n",
    "    lambda s: initiate_interviews(s) if s.get(\"human_analyst_feedback\") == \"OK\" else \"create_analysts\",\n",
    "    {\"conduct_interview\": \"conduct_interview\", \"create_analysts\": \"create_analysts\"})\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"write_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277fa65-0378-4d3e-9107-f61407fd454e",
   "metadata": {},
   "source": [
    "## Provide Approval and Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a609d-ce3b-436c-8323-a654da6fbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"Harris_Research_v2\"}, \"recursion_limit\": 50}\n",
    "\n",
    "# 1. Start Analysis\n",
    "initial_input = {\"topic\": \"Best practices for LangGraph parallelization\", \"max_analysts\": 3}\n",
    "for event in graph.stream(initial_input, config, stream_mode=\"values\"):\n",
    "    pass\n",
    "\n",
    "# 2. Approve Analysts (Set to OK to proceed)\n",
    "graph.update_state(config, {\"human_analyst_feedback\": \"OK\"}, as_node=\"human_feedback\")\n",
    "\n",
    "# 3. Resume with Research (Parallel Processing)\n",
    "print(\"\\nStarting research and interviews...\")\n",
    "for event in graph.stream(None, config, stream_mode=\"updates\", max_concurrency=1):\n",
    "    node = list(event.keys())[0]\n",
    "    print(f\"‚úÖ Node {node} completed.\")\n",
    "\n",
    "# 4. Final Display\n",
    "final_data = graph.get_state(config).values\n",
    "display(Markdown(final_data.get('final_report', \"Report failed to generate.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d4cf4-a205-4c19-b605-eb8a4b0860ad",
   "metadata": {},
   "source": [
    "### 4. Professional Formatted Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdca68-c111-43b8-a357-02bb0d300fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the report from the final state of the graph\n",
    "final_state = graph.get_state(config)\n",
    "report_raw = final_state.values.get('final_report', '')\n",
    "\n",
    "# 2. Try to parse and format the JSON report\n",
    "try:\n",
    "    # Handle the case where the LLM wrapped it in Markdown code blocks\n",
    "    clean_json = report_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    data = json.loads(clean_json)\n",
    "    \n",
    "    # Check if 'report' is the top-level key (as seen in your output)\n",
    "    content = data.get('report', data)\n",
    "    \n",
    "    markdown_out = f\"# {content.get('title', 'Research Report')}\\n\\n\"\n",
    "    markdown_out += f\"## Introduction\\n{content.get('introduction', '')}\\n\\n\"\n",
    "    \n",
    "    for section in content.get('sections', []):\n",
    "        title = section.get('title') or \"Analysis\"\n",
    "        markdown_out += f\"### {title}\\n{section.get('content', '')}\\n\\n\"\n",
    "        \n",
    "    markdown_out += f\"## Conclusion\\n{content.get('conclusion', '')}\\n\\n\"\n",
    "    \n",
    "    if 'references' in content:\n",
    "        markdown_out += \"## References\\n* \" + \"\\n* \".join(content['references'])\n",
    "    \n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback: Just print the raw text if JSON parsing fails\n",
    "    print(\"Parsing failed or report already in Markdown. Displaying raw output:\")\n",
    "    display(Markdown(report_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824da862-a1b3-4ffb-b33c-da724d6b7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
