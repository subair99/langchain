{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61652-903a-45cf-b95b-1bfb77d774ec",
   "metadata": {},
   "source": [
    "## Core Logic and Graph Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0f7fd-796b-4fdc-ae75-7e708a9544d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "from typing import List, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "# Note: Ensure your TAVILY_API_KEY is set in your environment variables\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0)\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str; name: str; role: str; description: str\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    analyst: Analyst \n",
    "    context: Annotated[list, operator.add]\n",
    "    sections: list\n",
    "    sources: Annotated[list, operator.add] # New: Track URLs\n",
    "\n",
    "# --- 2. NODES ---\n",
    "def ask_question(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    sys_msg = f\"You are {analyst.name}, {analyst.role}. Ask a technical question about the topic.\"\n",
    "    return {\"messages\": [llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])]}\n",
    "\n",
    "def search_node(state: InterviewState):\n",
    "    \"\"\"Generates a search query and fetches live data from Tavily.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1].content\n",
    "    sys_msg = SystemMessage(content=\"Output ONLY a JSON object with a 'query' key for web search.\")\n",
    "    \n",
    "    # Generate query\n",
    "    res = llm.with_structured_output(SearchQuery).invoke([sys_msg, HumanMessage(content=last_msg)])\n",
    "    print(f\"ðŸ” {state['analyst'].name} is researching: {res.query}\")\n",
    "    \n",
    "    # Execute search\n",
    "    search_data = search_tool.invoke(res.query)\n",
    "    \n",
    "    content_list = []\n",
    "    source_links = []\n",
    "    for r in search_data:\n",
    "        content_list.append(f\"Source: {r['url']}\\nContent: {r['content']}\")\n",
    "        source_links.append(r['url'])\n",
    "        \n",
    "    return {\n",
    "        \"context\": [\"\\n\\n\".join(content_list)], \n",
    "        \"sources\": source_links\n",
    "    }\n",
    "\n",
    "def answer_node(state: InterviewState):\n",
    "    context_str = \"\\n\".join(state[\"context\"])\n",
    "    sys_msg = f\"Using this research:\\n{context_str}\\n\\nAnswer the analyst's question. Be technical.\"\n",
    "    res = llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])\n",
    "    return {\"messages\": [res], \"sections\": [res.content]}\n",
    "\n",
    "# --- 3. SUB-GRAPH BUILD ---\n",
    "itv_builder = StateGraph(InterviewState)\n",
    "itv_builder.add_node(\"ask\", ask_question)\n",
    "itv_builder.add_node(\"search\", search_node)\n",
    "itv_builder.add_node(\"answer\", answer_node)\n",
    "itv_builder.add_edge(START, \"ask\")\n",
    "itv_builder.add_edge(\"ask\", \"search\")\n",
    "itv_builder.add_edge(\"search\", \"answer\")\n",
    "itv_builder.add_edge(\"answer\", END)\n",
    "interview_graph = itv_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce4fd-e258-4617-b3a0-dda8ab2632f1",
   "metadata": {},
   "source": [
    "## Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a6c77a-f518-4fed-bfa8-c5f9d87b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str; max_analysts: int; human_analyst_feedback: str\n",
    "    analysts: List[Analyst]; sections: Annotated[list, operator.add]\n",
    "    sources: Annotated[list, operator.add]; final_report: str\n",
    "\n",
    "def create_analysts(state: ResearchState):\n",
    "    prompt = f\"Create a team of {state['max_analysts']} analysts for: {state['topic']}. Return JSON.\"\n",
    "    res = llm.with_structured_output(Perspectives).invoke([SystemMessage(content=prompt)])\n",
    "    \n",
    "    # PRINTING IN YOUR REQUESTED FORMAT\n",
    "    print(\"\\n--- ANALYST TEAM GENERATED ---\")\n",
    "    for a in res.analysts:\n",
    "        print(f\"Name: {a.name}\")\n",
    "        print(f\"Affiliation: {a.affiliation}\")\n",
    "        print(f\"Role: {a.role}\")\n",
    "        print(f\"Description: {a.description}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return {\"analysts\": res.analysts}\n",
    "\n",
    "def initiate_interviews(state: ResearchState):\n",
    "    return [Send(\"conduct_interview\", {\"analyst\": a, \"messages\": [HumanMessage(content=state[\"topic\"])]}) for a in state[\"analysts\"]]\n",
    "\n",
    "def compile_report(state: ResearchState):\n",
    "    all_sections = \"\\n\\n\".join(state[\"sections\"])\n",
    "    # Format sources as a clean Markdown list\n",
    "    unique_sources = list(set(state.get(\"sources\", [])))\n",
    "    source_str = \"\\n\".join([f\"* [{s}]({s})\" for s in unique_sources])\n",
    "    \n",
    "    prompt = f\"Write a professional report using these sections:\\n{all_sections}\\n\\nAt the very end, add a section called '### References' and list these exact links: {source_str}\"\n",
    "    res = llm.invoke(prompt)\n",
    "    return {\"final_report\": res.content}\n",
    "\n",
    "# GRAPH BUILDER\n",
    "builder = StateGraph(ResearchState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", lambda s: None)\n",
    "builder.add_node(\"conduct_interview\", interview_graph)\n",
    "builder.add_node(\"write_report\", compile_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", \n",
    "    lambda s: initiate_interviews(s) if s.get(\"human_analyst_feedback\") == \"OK\" else \"create_analysts\",\n",
    "    {\"conduct_interview\": \"conduct_interview\", \"create_analysts\": \"create_analysts\"})\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"write_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277fa65-0378-4d3e-9107-f61407fd454e",
   "metadata": {},
   "source": [
    "## Provide Approval and Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923a609d-ce3b-436c-8323-a654da6fbe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANALYST TEAM GENERATED ---\n",
      "Name: Dr. Elena Voss\n",
      "Affiliation: Distributed Systems Lab\n",
      "Role: Parallelization Architect\n",
      "Description: Specializes in distributed computing and graph processing frameworks. Focuses on optimizing LangGraph's architecture for parallel execution, including node partitioning, communication overhead reduction, and fault tolerance strategies.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Raj Patel\n",
      "Affiliation: AI Performance Optimization Group\n",
      "Role: Optimization Lead\n",
      "Description: Expert in algorithmic efficiency and resource allocation. Leads efforts to balance workloads, minimize contention, and leverage hardware-specific optimizations (e.g., GPU/TPU utilization) for LangGraph parallelization.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Aisha Kim\n",
      "Affiliation: Machine Learning Systems Team\n",
      "Role: Performance Analyst\n",
      "Description: Focuses on benchmarking and monitoring LangGraph's parallel execution. Develops metrics for latency, throughput, and scalability, and identifies bottlenecks through profiling and A/B testing of parallelization strategies.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Send' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Approve Analysts (Set to OK to proceed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_analyst_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOK\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 3. Resume with Research (Parallel Processing)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting research and interviews...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2320\u001b[39m, in \u001b[36mPregel.update_state\u001b[39m\u001b[34m(self, config, values, as_node, task_id)\u001b[39m\n\u001b[32m   2309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_state\u001b[39m(\n\u001b[32m   2310\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2311\u001b[39m     config: RunnableConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2314\u001b[39m     task_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2315\u001b[39m ) -> RunnableConfig:\n\u001b[32m   2316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update the state of the graph with the given values, as if they came from\u001b[39;00m\n\u001b[32m   2317\u001b[39m \u001b[33;03m    node `as_node`. If `as_node` is not provided, it will be set to the last node\u001b[39;00m\n\u001b[32m   2318\u001b[39m \u001b[33;03m    that updated the state, if not ambiguous.\u001b[39;00m\n\u001b[32m   2319\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbulk_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mStateUpdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:1866\u001b[39m, in \u001b[36mPregel.bulk_update_state\u001b[39m\u001b[34m(self, config, supersteps)\u001b[39m\n\u001b[32m   1862\u001b[39m current_config = patch_configurable(\n\u001b[32m   1863\u001b[39m     config, {CONFIG_KEY_THREAD_ID: \u001b[38;5;28mstr\u001b[39m(config[CONF][CONFIG_KEY_THREAD_ID])}\n\u001b[32m   1864\u001b[39m )\n\u001b[32m   1865\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m superstep \u001b[38;5;129;01min\u001b[39;00m supersteps:\n\u001b[32m-> \u001b[39m\u001b[32m1866\u001b[39m     current_config = \u001b[43mperform_superstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuperstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:1801\u001b[39m, in \u001b[36mPregel.bulk_update_state.<locals>.perform_superstep\u001b[39m\u001b[34m(input_config, updates)\u001b[39m\n\u001b[32m   1799\u001b[39m     run = RunnableSequence(*writers) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(writers) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m writers[\u001b[32m0\u001b[39m]\n\u001b[32m   1800\u001b[39m     \u001b[38;5;66;03m# execute task\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1801\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdateState\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# deque.extend is thread-safe\u001b[39;49;00m\n\u001b[32m   1808\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_TASK_ID\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m                \u001b[49m\u001b[43mCONFIG_KEY_READ\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m_scratchpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m                        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmanaged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[38;5;66;03m# save task writes\u001b[39;00m\n\u001b[32m   1829\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_id, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_task_ids, run_tasks):\n\u001b[32m   1830\u001b[39m     \u001b[38;5;66;03m# channel writes are saved to current checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/graph/_branch.py:166\u001b[39m, in \u001b[36mBranchSpec._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/3-langgraph_intro/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:393\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    391\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    395\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     41\u001b[39m builder.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33mcreate_analysts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mcreate_analysts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhuman_feedback\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m builder.add_conditional_edges(\u001b[33m\"\u001b[39m\u001b[33mhuman_feedback\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[43minitiate_interviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m s.get(\u001b[33m\"\u001b[39m\u001b[33mhuman_analyst_feedback\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mOK\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcreate_analysts\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mconduct_interview\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mconduct_interview\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcreate_analysts\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcreate_analysts\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     46\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mconduct_interview\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwrite_report\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mwrite_report\u001b[39m\u001b[33m\"\u001b[39m, END)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36minitiate_interviews\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitiate_interviews\u001b[39m(state: ResearchState):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mSend\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mconduct_interview\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33manalyst\u001b[39m\u001b[33m\"\u001b[39m: a, \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=state[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m])]}) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33manalysts\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'Send' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"Harris_Research_v2\"}, \"recursion_limit\": 50}\n",
    "\n",
    "# 1. Start Analysis\n",
    "initial_input = {\"topic\": \"Best practices for LangGraph parallelization\", \"max_analysts\": 3}\n",
    "for event in graph.stream(initial_input, config, stream_mode=\"values\"):\n",
    "    pass\n",
    "\n",
    "# 2. Approve Analysts (Set to OK to proceed)\n",
    "graph.update_state(config, {\"human_analyst_feedback\": \"OK\"}, as_node=\"human_feedback\")\n",
    "\n",
    "# 3. Resume with Research (Parallel Processing)\n",
    "print(\"\\nStarting research and interviews...\")\n",
    "for event in graph.stream(None, config, stream_mode=\"updates\", max_concurrency=1):\n",
    "    node = list(event.keys())[0]\n",
    "    print(f\"âœ… Node {node} completed.\")\n",
    "\n",
    "# 4. Final Display\n",
    "final_data = graph.get_state(config).values\n",
    "display(Markdown(final_data.get('final_report', \"Report failed to generate.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d4cf4-a205-4c19-b605-eb8a4b0860ad",
   "metadata": {},
   "source": [
    "### 4. Professional Formatted Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdca68-c111-43b8-a357-02bb0d300fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the report from the final state of the graph\n",
    "final_state = graph.get_state(config)\n",
    "report_raw = final_state.values.get('final_report', '')\n",
    "\n",
    "# 2. Try to parse and format the JSON report\n",
    "try:\n",
    "    # Handle the case where the LLM wrapped it in Markdown code blocks\n",
    "    clean_json = report_raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    data = json.loads(clean_json)\n",
    "    \n",
    "    # Check if 'report' is the top-level key (as seen in your output)\n",
    "    content = data.get('report', data)\n",
    "    \n",
    "    markdown_out = f\"# {content.get('title', 'Research Report')}\\n\\n\"\n",
    "    markdown_out += f\"## Introduction\\n{content.get('introduction', '')}\\n\\n\"\n",
    "    \n",
    "    for section in content.get('sections', []):\n",
    "        title = section.get('title') or \"Analysis\"\n",
    "        markdown_out += f\"### {title}\\n{section.get('content', '')}\\n\\n\"\n",
    "        \n",
    "    markdown_out += f\"## Conclusion\\n{content.get('conclusion', '')}\\n\\n\"\n",
    "    \n",
    "    if 'references' in content:\n",
    "        markdown_out += \"## References\\n* \" + \"\\n* \".join(content['references'])\n",
    "    \n",
    "    display(Markdown(markdown_out))\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback: Just print the raw text if JSON parsing fails\n",
    "    print(\"Parsing failed or report already in Markdown. Displaying raw output:\")\n",
    "    display(Markdown(report_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824da862-a1b3-4ffb-b33c-da724d6b7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
