{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d61652-903a-45cf-b95b-1bfb77d774ec",
   "metadata": {},
   "source": [
    "## Core Logic and Graph Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d0f7fd-796b-4fdc-ae75-7e708a9544d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import json\n",
    "import datetime\n",
    "from typing import List, Annotated, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "llm = ChatOllama(model=\"qwen3:8b\", temperature=0)\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "# Logging Helper\n",
    "def log_research(analyst_name, query, sources):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] Analyst: {analyst_name}\\nQuery: {query}\\nSources: {', '.join(sources)}\\n\"\n",
    "    with open(\"research_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(log_entry + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# --- 2. SCHEMAS & STATE ---\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str; name: str; role: str; description: str\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst]\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    analyst: Analyst \n",
    "    context: Annotated[list, operator.add]\n",
    "    sections: list\n",
    "    sources: Annotated[list, operator.add]\n",
    "\n",
    "# --- 3. ROBUST NODES ---\n",
    "\n",
    "def ask_question(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    sys_msg = f\"You are {analyst.name}, {analyst.role}. Ask a targeted technical question.\"\n",
    "    return {\"messages\": [llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])]}\n",
    "\n",
    "def search_node(state: InterviewState):\n",
    "    last_msg = state[\"messages\"][-1].content\n",
    "    sys_msg = SystemMessage(content=\"Output ONLY a JSON object with a 'query' key.\")\n",
    "    \n",
    "    try:\n",
    "        structured_llm = llm.with_structured_output(SearchQuery)\n",
    "        res = structured_llm.invoke([sys_msg, HumanMessage(content=last_msg)])\n",
    "        query_text = res.query\n",
    "    except Exception:\n",
    "        query_text = f\"{state['analyst'].role} {last_msg[:50]}\"\n",
    "\n",
    "    print(f\"üîç {state['analyst'].name} researching: {query_text}\")\n",
    "    \n",
    "    search_data = search_tool.invoke(query_text)\n",
    "    \n",
    "    content_list = []\n",
    "    source_links = []\n",
    "\n",
    "    if isinstance(search_data, list):\n",
    "        for r in search_data:\n",
    "            if isinstance(r, dict) and 'url' in r and 'content' in r:\n",
    "                content_list.append(f\"Source: {r['url']}\\nContent: {r['content']}\")\n",
    "                source_links.append(r['url'])\n",
    "    \n",
    "    # NEW: Log the findings to a file\n",
    "    log_research(state['analyst'].name, query_text, source_links)\n",
    "        \n",
    "    return {\n",
    "        \"context\": [\"\\n\\n\".join(content_list)], \n",
    "        \"sources\": source_links\n",
    "    }\n",
    "\n",
    "def answer_node(state: InterviewState):\n",
    "    context_str = \"\\n\".join(state[\"context\"])\n",
    "    sys_msg = f\"Using ONLY the provided context, answer the question technicaly.\\n\\nCONTEXT:\\n{context_str}\"\n",
    "    res = llm.invoke([SystemMessage(content=sys_msg)] + state[\"messages\"])\n",
    "    res.name = \"expert\"\n",
    "    return {\"messages\": [res], \"sections\": [res.content]}\n",
    "\n",
    "# --- 4. BUILD SUB-GRAPH ---\n",
    "itv_builder = StateGraph(InterviewState)\n",
    "itv_builder.add_node(\"ask\", ask_question)\n",
    "itv_builder.add_node(\"search\", search_node)\n",
    "itv_builder.add_node(\"answer\", answer_node)\n",
    "itv_builder.add_edge(START, \"ask\")\n",
    "itv_builder.add_edge(\"ask\", \"search\")\n",
    "itv_builder.add_edge(\"search\", \"answer\")\n",
    "itv_builder.add_edge(\"answer\", END)\n",
    "interview_graph = itv_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ce4fd-e258-4617-b3a0-dda8ab2632f1",
   "metadata": {},
   "source": [
    "## Execute Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a6c77a-f518-4fed-bfa8-c5f9d87b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. STATE DEFINITIONS ---\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    human_analyst_feedback: str\n",
    "    analysts: List[Analyst] # Defined in Cell 1\n",
    "    sections: Annotated[list, operator.add]\n",
    "    sources: Annotated[list, operator.add]\n",
    "    final_report: str\n",
    "\n",
    "# --- 2. ORCHESTRATOR FUNCTIONS ---\n",
    "\n",
    "def create_analysts(state: ResearchState):\n",
    "    # Optional: Clear logs at the start of a fresh analyst creation\n",
    "    if os.path.exists(\"research_log.txt\"):\n",
    "        os.remove(\"research_log.txt\")\n",
    "        \n",
    "    prompt = f\"Create a team of {state['max_analysts']} analysts for: {state['topic']}. Return JSON.\"\n",
    "    res = llm.with_structured_output(Perspectives).invoke([SystemMessage(content=prompt)])\n",
    "    \n",
    "    print(\"\\n--- ANALYST TEAM GENERATED ---\")\n",
    "    for a in res.analysts:\n",
    "        print(f\"Name: {a.name}\")\n",
    "        print(f\"Affiliation: {a.affiliation}\")\n",
    "        print(f\"Role: {a.role}\")\n",
    "        print(f\"Description: {a.description}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    return {\"analysts\": res.analysts}\n",
    "\n",
    "def initiate_interviews(state: ResearchState):\n",
    "    return [Send(\"conduct_interview\", {\"analyst\": a, \"messages\": [HumanMessage(content=state[\"topic\"])]}) for a in state[\"analysts\"]]\n",
    "\n",
    "def compile_report(state: ResearchState):\n",
    "    all_sections = \"\\n\\n\".join(state[\"sections\"])\n",
    "    \n",
    "    # Read logs for methodology section\n",
    "    try:\n",
    "        with open(\"research_log.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_logs = f.read()\n",
    "    except FileNotFoundError:\n",
    "        raw_logs = \"No log data found.\"\n",
    "\n",
    "    unique_sources = list(set(state.get(\"sources\", [])))\n",
    "    source_str = \"\\n\".join([f\"* [{s}]({s})\" for s in unique_sources])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Write a comprehensive technical report based on these sections:\n",
    "    {all_sections}\n",
    "    \n",
    "    Research Methodology (extracted from logs):\n",
    "    {raw_logs[-2000:]}\n",
    "    \n",
    "    Structure:\n",
    "    1. Executive Summary\n",
    "    2. Detailed Findings\n",
    "    3. Research Methodology (mention queries used)\n",
    "    4. References (use these links: {source_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"‚úçÔ∏è  Compiling final report with log summary...\")\n",
    "    res = llm.invoke(prompt)\n",
    "    return {\"final_report\": res.content}\n",
    "\n",
    "# --- 3. GRAPH CONSTRUCTION ---\n",
    "builder = StateGraph(ResearchState)\n",
    "\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", lambda s: None)\n",
    "builder.add_node(\"conduct_interview\", interview_graph) # interview_graph from Cell 1\n",
    "builder.add_node(\"write_report\", compile_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", \n",
    "    lambda s: initiate_interviews(s) if s.get(\"human_analyst_feedback\") == \"OK\" else \"create_analysts\",\n",
    "    {\"conduct_interview\": \"conduct_interview\", \"create_analysts\": \"create_analysts\"}\n",
    ")\n",
    "\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"write_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277fa65-0378-4d3e-9107-f61407fd454e",
   "metadata": {},
   "source": [
    "## Provide Approval and Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923a609d-ce3b-436c-8323-a654da6fbe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANALYST TEAM GENERATED ---\n",
      "Name: Dr. Elena Voss\n",
      "Affiliation: Distributed Systems Lab\n",
      "Role: Parallelization Architect\n",
      "Description: Specializes in distributed computing and graph processing frameworks. Focuses on optimizing LangGraph's architecture for parallel execution, including node partitioning, communication overhead reduction, and fault tolerance strategies.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Raj Patel\n",
      "Affiliation: AI Performance Optimization Group\n",
      "Role: Optimization Lead\n",
      "Description: Expert in algorithmic efficiency and resource allocation. Leads efforts to balance workloads, minimize contention, and leverage hardware-specific optimizations (e.g., GPU/TPU utilization) for LangGraph parallelization.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Aisha Kim\n",
      "Affiliation: Machine Learning Systems Team\n",
      "Role: Performance Analyst\n",
      "Description: Focuses on benchmarking and monitoring LangGraph's parallel execution. Develops metrics for latency, throughput, and scalability, and identifies bottlenecks through profiling and A/B testing of parallelization strategies.\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting research and interviews...\n",
      "üîç Dr. Elena Voss researching: When implementing data parallelism in LangGraph for distributed training on heterogeneous hardware (e.g., mixed GPU/TPU clusters), optimizing gradient synchronization latency while maintaining model accuracy involves addressing three key areas: (1) **Communication Overhead**: Use asynchronous updates with gradient accumulation to reduce sync frequency, combined with device-specific AllReduce optimizations (e.g., TPU-aware collective operations) and overlapping communication with computation. (2) **Device-Aware Sharding**: Dynamically partition model parameters/data based on device computational capacity, prioritizing high-capacity devices for compute-intensive layers and balancing workloads via runtime monitoring and adaptive sharding strategies. (3) **Fault Tolerance**: Implement straggler-tolerant synchronization (e.g., asynchronous consensus protocols) and partial failure recovery mechanisms, such as checkpointing and redundant gradient aggregation, to ensure convergence despite hardware variability. Trade-offs between latency, accuracy, and robustness are managed through hybrid strategies that adapt to real-time hardware performance metrics.\n",
      "üîç Dr. Raj Patel researching: When implementing LangGraph parallelization, how do you balance the trade-offs between data parallelism (batch size scaling) and model parallelism (layer/shard distribution) to optimize both training throughput and memory efficiency, especially for large-scale language models?\n",
      "üîç Dr. Aisha Kim researching: When parallelizing LangGraph workflows, optimize node-level dependencies by combining data parallelism (splitting input batches across workers like Ray/dask) and model parallelism (partitioning computational nodes while preserving dependency chains). Use dependency graph analysis (e.g., graphviz/networkx) to identify independent nodes for parallel execution, leverage asynchronous I/O (async/await) for non-blocking operations, and implement dynamic load balancing (e.g., Ray's custom scheduling). Minimize communication overhead via localized intermediate outputs (shared memory/torch.distributed) and ensure data consistency with checkpointing for fault tolerance. Example: Parallelize independent query processing and embedding generation using Ray, ensuring embedding nodes wait for input data via explicit dependencies.\n",
      "‚úÖ Node conduct_interview completed.\n",
      "‚úÖ Node conduct_interview completed.\n",
      "‚úÖ Node conduct_interview completed.\n",
      "‚úçÔ∏è  Compiling final report with log summary...\n",
      "‚úÖ Node write_report completed.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# **Technical Report: Optimizing LangGraph Parallelization for Heterogeneous Hardware**  \n",
       "\n",
       "---\n",
       "\n",
       "## **1. Executive Summary**  \n",
       "This report provides a comprehensive analysis of strategies to optimize LangGraph parallelization, focusing on balancing **communication overhead**, **hardware heterogeneity**, and **model efficiency**. Key recommendations include:  \n",
       "- **Communication Optimization**: Gradient compression, asynchronous updates, and hierarchical AllReduce to reduce latency.  \n",
       "- **Device-Aware Sharding**: Dynamic partitioning of model parameters/data based on device capabilities (e.g., GPU vs. TPU).  \n",
       "- **Fault Tolerance**: Straggler mitigation, checkpointing, and fault-resilient synchronization to ensure robustness.  \n",
       "- **Combined Strategies**: Hybrid parallelism, pipeline parallelism, and mixed precision to balance scalability and performance.  \n",
       "\n",
       "The report also outlines **technical trade-offs** and **implementation considerations** for deploying LangGraph on heterogeneous hardware, ensuring optimal throughput, memory efficiency, and fault tolerance.  \n",
       "\n",
       "---\n",
       "\n",
       "## **2. Detailed Findings**  \n",
       "\n",
       "### **1. Communication Overhead Optimization**  \n",
       "**Key Techniques**:  \n",
       "- **Gradient Accumulation**: Reduces the frequency of AllReduce operations, mitigating latency from heterogeneous hardware.  \n",
       "- **Asynchronous Updates (Async SGD)**: Overlaps computation and communication, reducing idle time on faster devices.  \n",
       "- **Hierarchical AllReduce**: Multi-level reduction (e.g., device-group ‚Üí global) minimizes cross-device bandwidth usage.  \n",
       "- **Gradient Compression**: Quantization or sparsification (e.g., Top-k, SignSGD) reduces data transfer size, critical for TPUs with limited bandwidth.  \n",
       "\n",
       "**Trade-offs**:  \n",
       "- Gradient compression may slightly reduce accuracy but significantly lowers latency.  \n",
       "- Asynchronous updates introduce potential staleness in gradients, requiring careful hyperparameter tuning.  \n",
       "\n",
       "**Example**: For TPUs with limited bandwidth, use **Top-k gradient sparsification** to reduce data transfer by 50% while maintaining 95% model accuracy.  \n",
       "\n",
       "---\n",
       "\n",
       "### **2. Device-Aware Sharding**  \n",
       "**Key Techniques**:  \n",
       "- **Dynamic Load Balancing**: Partition model parameters/data based on device capabilities (e.g., assign larger partitions to high-memory GPUs).  \n",
       "- **Pipeline Parallelism**: Combines data and model parallelism to balance computation and communication.  \n",
       "- **Model Offloading**: Offload non-critical layers (e.g., attention heads) to lower-capacity devices.  \n",
       "\n",
       "**Trade-offs**:  \n",
       "- Device-aware sharding increases implementation complexity but improves resource utilization.  \n",
       "- Pipeline parallelism requires careful scheduling to avoid idle time on underutilized devices.  \n",
       "\n",
       "**Example**: For a 100B-parameter model, assign compute-heavy layers to GPUs and offload attention heads to TPUs, ensuring balanced workloads.  \n",
       "\n",
       "---\n",
       "\n",
       "### **3. Fault Tolerance Mechanisms**  \n",
       "**Key Techniques**:  \n",
       "- **Straggler Mitigation**: Gradient averaging or redundant synchronization (e.g., duplicate gradients from fast devices).  \n",
       "- **Checkpointing**: Periodically save model states to recover from partial failures.  \n",
       "- **Fault-Resilient AllReduce**: Use checksums or erasure coding to detect and recover from partial failures.  \n",
       "\n",
       "**Trade-offs**:  \n",
       "- Fault tolerance mechanisms add overhead but ensure convergence in heterogeneous clusters.  \n",
       "- Checkpointing increases storage and computational costs.  \n",
       "\n",
       "**Example**: Use **gradient averaging** to tolerate slow devices in a TPU-GPU cluster, reducing recovery time by 40%.  \n",
       "\n",
       "---\n",
       "\n",
       "### **4. Combined Strategies**  \n",
       "**Hybrid Parallelism**:  \n",
       "- Combine data and model parallelism for large models (e.g., 100B+ parameters). Use data parallelism for smaller sub-models and model parallelism for distributed layers.  \n",
       "\n",
       "**Pipeline Parallelism**:  \n",
       "- Split the model into stages (e.g., 4 stages) and interleave data batches across stages to hide communication latency.  \n",
       "\n",
       "**Gradient Checkpointing & Mixed Precision**:  \n",
       "- Use gradient checkpointing to trade memory for computation (e.g., store activations at checkpoints instead of all layers).  \n",
       "- Combine with mixed precision (FP16/FP8) to reduce memory usage and improve throughput.  \n",
       "\n",
       "**Example**: For a 500B-parameter model, use pipeline parallelism with gradient checkpointing to reduce memory usage by 60% while maintaining 90% throughput.  \n",
       "\n",
       "---\n",
       "\n",
       "## **3. Research Methodology**  \n",
       "The findings are derived from the following queries and analyses:  \n",
       "\n",
       "### **Query 1: Dr. Raj Patel**  \n",
       "**Focus**: Balancing data parallelism (batch size scaling) and model parallelism (layer/shard distribution) for large-scale language models.  \n",
       "**Key Contributions**:  \n",
       "- Highlighted the trade-offs between latency, accuracy, and robustness.  \n",
       "- Proposed hybrid strategies to optimize throughput and memory efficiency.  \n",
       "\n",
       "### **Query 2: Dr. Aisha Kim**  \n",
       "**Focus**: Optimizing node-level dependencies in LangGraph workflows.  \n",
       "**Key Contributions**:  \n",
       "- Emphasized the use of dependency graph analysis (e.g., graphviz/networkx) to identify independent nodes for parallel execution.  \n",
       "- Advocated for asynchronous I/O (async/await) and dynamic load balancing (e.g., Ray‚Äôs custom scheduling).  \n",
       "\n",
       "### **Query 3: Hardware Heterogeneity**  \n",
       "**Focus**: Adapting LangGraph to heterogeneous hardware (e.g., TPUs, GPUs).  \n",
       "**Key Contributions**:  \n",
       "- Introduced device-aware sharding and fault-resilient synchronization techniques.  \n",
       "- Demonstrated the importance of runtime monitoring for adaptive sharding strategies.  \n",
       "\n",
       "---\n",
       "\n",
       "## **4. References**  \n",
       "1. [Dr. Raj Patel‚Äôs Query on Trade-offs](https://example.com/raj-patel-query)  \n",
       "2. [Dr. Aisha Kim‚Äôs Query on Node-Level Dependencies](https://example.com/aisha-kim-query)  \n",
       "3. [Hardware Heterogeneity Analysis](https://example.com/hardware-heterogeneity)  \n",
       "\n",
       "---  \n",
       "**End of Report**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"Harris_Research_v2\"}, \"recursion_limit\": 50}\n",
    "\n",
    "# 1. Start Analysis\n",
    "initial_input = {\"topic\": \"Best practices for LangGraph parallelization\", \"max_analysts\": 3}\n",
    "for event in graph.stream(initial_input, config, stream_mode=\"values\"):\n",
    "    pass\n",
    "\n",
    "# 2. Approve Analysts (Set to OK to proceed)\n",
    "graph.update_state(config, {\"human_analyst_feedback\": \"OK\"}, as_node=\"human_feedback\")\n",
    "\n",
    "# 3. Resume with Research (Parallel Processing)\n",
    "print(\"\\nStarting research and interviews...\")\n",
    "for event in graph.stream(None, config, stream_mode=\"updates\", max_concurrency=1):\n",
    "    node = list(event.keys())[0]\n",
    "    print(f\"‚úÖ Node {node} completed.\")\n",
    "\n",
    "# 4. Final Display\n",
    "final_data = graph.get_state(config).values\n",
    "display(Markdown(final_data.get('final_report', \"Report failed to generate.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824da862-a1b3-4ffb-b33c-da724d6b7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nice one "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
