{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc65774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaa61b",
   "metadata": {},
   "source": [
    "## Setup Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079e293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"travel_server\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"https://mcp.kiwi.com\"\n",
    "            }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970310e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "from langchain.tools import tool\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3431ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///resources/Chinook.db\")\n",
    "\n",
    "@tool\n",
    "def query_playlist_db(query: str) -> str:\n",
    "\n",
    "    \"\"\"Query the database for playlist information\"\"\"\n",
    "\n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error querying database: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfef74",
   "metadata": {},
   "source": [
    "## Create State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4bf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "class WeddingState(AgentState):\n",
    "    origin: str\n",
    "    destination: str\n",
    "    guest_count: str\n",
    "    genre: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791bb9f",
   "metadata": {},
   "source": [
    "## Create Subagents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301bed37-eacb-4b44-94df-86649b03e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"nemotron-super-49b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fffb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# Travel agent\n",
    "travel_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"\n",
    "    You are a travel agent. Search for flights to the desired destination wedding location.\n",
    "    You are not allowed to ask any more follow up questions, you must find the best flight options based on the following criteria:\n",
    "    - Price (lowest, economy class)\n",
    "    - Duration (shortest)\n",
    "    - Date (time of year which you believe is best for a wedding at this location)\n",
    "    To make things easy, only look for one ticket, one way.\n",
    "    You may need to make multiple searches to iteratively find the best options.\n",
    "    You will be given no extra information, only the origin and destination. It is your job to think critically about the best options.\n",
    "    Once you have found the best options, let the user know your shortlist of options.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c82ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venue agent\n",
    "venue_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[web_search],\n",
    "    system_prompt=\"\"\"\n",
    "    You are a venue specialist. Search for venues in the desired location, and with the desired capacity.\n",
    "    You are not allowed to ask any more follow up questions, you must find the best venue options based on the following criteria:\n",
    "    - Price (lowest)\n",
    "    - Capacity (exact match)\n",
    "    - Reviews (highest)\n",
    "    You may need to make multiple searches to iteratively find the best options.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df817409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playlist agent\n",
    "playlist_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[query_playlist_db],\n",
    "    system_prompt=\"\"\"\n",
    "    You are a playlist specialist. Query the sql database and curate the perfect playlist for a wedding given a genre.\n",
    "    Once you have your playlist, calculate the total duration and cost of the playlist, each song has an associated price.\n",
    "    If you run into errors when querying the database, try to fix them by making changes to the query.\n",
    "    Do not come back empty handed, keep trying to query the db until you find a list of songs.\n",
    "    You may need to make multiple queries to iteratively find the best options.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb3fe6",
   "metadata": {},
   "source": [
    "## Main Coordinator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0603a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import ToolRuntime\n",
    "from langchain.messages import HumanMessage, ToolMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool\n",
    "async def search_flights(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Travel agent searches for flights to the desired destination wedding location.\"\"\"\n",
    "    origin = runtime.state[\"origin\"]\n",
    "    destination = runtime.state[\"destination\"]\n",
    "    response = await travel_agent.ainvoke({\"messages\": [HumanMessage(content=f\"Find flights from {origin} to {destination}\")]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def search_venues(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Venue agent chooses the best venue for the given location and capacity.\"\"\"\n",
    "    destination = runtime.state[\"destination\"]\n",
    "    capacity = runtime.state[\"guest_count\"]\n",
    "    query = f\"Find wedding venues in {destination} for {capacity} guests\"\n",
    "    response = venue_agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def suggest_playlist(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Playlist agent curates the perfect playlist for the given genre.\"\"\"\n",
    "    genre = runtime.state[\"genre\"]\n",
    "    query = f\"Find {genre} tracks for wedding playlist\"\n",
    "    response = playlist_agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def update_state(origin: str, destination: str, guest_count: str, genre: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Update the state when you know all of the values: origin, destination, guest_count, genre\"\"\"\n",
    "    return Command(update={\n",
    "        \"origin\": origin, \n",
    "        \"destination\": destination, \n",
    "        \"guest_count\": guest_count, \n",
    "        \"genre\": genre, \n",
    "        \"messages\": [ToolMessage(\"Successfully updated state\", tool_call_id=runtime.tool_call_id)]}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfa82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "coordinator = create_agent(\n",
    "    model=model,\n",
    "    tools=[search_flights, search_venues, suggest_playlist, update_state],\n",
    "    state_schema=WeddingState,\n",
    "    system_prompt=\"\"\"\n",
    "    You are a wedding coordinator. Delegate tasks to your specialists for flights, venues and playlists.\n",
    "    First find all the information you need to update the state. Once that is done you can delegate the tasks.\n",
    "    Once you have received their answers, coordinate the perfect wedding for me.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b21c80",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c7141c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `nemotron-super-49b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m coordinator.ainvoke(\n\u001b[32m      4\u001b[39m     {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm from London and I\u001b[39m\u001b[33m'\u001b[39m\u001b[33md like a wedding in Paris for 100 guests, jazz-genre\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m      6\u001b[39m     }\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1186\u001b[39m, in \u001b[36mcreate_agent.<locals>.amodel_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1173\u001b[39m request = ModelRequest(\n\u001b[32m   1174\u001b[39m     model=model,\n\u001b[32m   1175\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1181\u001b[39m     runtime=runtime,\n\u001b[32m   1182\u001b[39m )\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m awrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m# No async handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _execute_model_async(request)\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1188\u001b[39m     \u001b[38;5;66;03m# Call composed async handler with base handler\u001b[39;00m\n\u001b[32m   1189\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m awrap_model_call_handler(request, _execute_model_async)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain/agents/factory.py:1157\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_async\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1155\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1157\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m model_.ainvoke(messages)\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1159\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5704\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5697\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5698\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5699\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5702\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5703\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5705\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5706\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5707\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5708\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:425\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     **kwargs: Any,\n\u001b[32m    423\u001b[39m ) -> AIMessage:\n\u001b[32m    424\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    426\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    427\u001b[39m         stop=stop,\n\u001b[32m    428\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    429\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    430\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    431\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    432\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    433\u001b[39m         **kwargs,\n\u001b[32m    434\u001b[39m     )\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    436\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1132\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1129\u001b[39m     **kwargs: Any,\n\u001b[32m   1130\u001b[39m ) -> LLMResult:\n\u001b[32m   1131\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1133\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1090\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1079\u001b[39m             *[\n\u001b[32m   1080\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m             ]\n\u001b[32m   1089\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m flattened_outputs = [\n\u001b[32m   1092\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1094\u001b[39m ]\n\u001b[32m   1095\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1359\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1357\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1360\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:642\u001b[39m, in \u001b[36mChatGroq._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    638\u001b[39m params = {\n\u001b[32m    639\u001b[39m     **params,\n\u001b[32m    640\u001b[39m     **kwargs,\n\u001b[32m    641\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(messages=message_dicts, **params)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:941\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    722\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    723\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    781\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m    782\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    784\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    939\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    942\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    943\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    944\u001b[39m             {\n\u001b[32m    945\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    946\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    947\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcitation_options\u001b[39m\u001b[33m\"\u001b[39m: citation_options,\n\u001b[32m    948\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mcompound_custom\u001b[39m\u001b[33m\"\u001b[39m: compound_custom,\n\u001b[32m    949\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdisable_tool_validation\u001b[39m\u001b[33m\"\u001b[39m: disable_tool_validation,\n\u001b[32m    950\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents,\n\u001b[32m    951\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mexclude_domains\u001b[39m\u001b[33m\"\u001b[39m: exclude_domains,\n\u001b[32m    952\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    953\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    954\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    955\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_domains\u001b[39m\u001b[33m\"\u001b[39m: include_domains,\n\u001b[32m    956\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude_reasoning\u001b[39m\u001b[33m\"\u001b[39m: include_reasoning,\n\u001b[32m    957\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    960\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    961\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    962\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    963\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    964\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    965\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m    966\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    967\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    968\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msearch_settings\u001b[39m\u001b[33m\"\u001b[39m: search_settings,\n\u001b[32m    969\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    970\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    971\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    972\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m    973\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    974\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    975\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    976\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    977\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    978\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    979\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    980\u001b[39m             },\n\u001b[32m    981\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m    982\u001b[39m         ),\n\u001b[32m    983\u001b[39m         options=make_request_options(\n\u001b[32m    984\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    985\u001b[39m         ),\n\u001b[32m    986\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    987\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    988\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    989\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/groq/_base_client.py:1762\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1750\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1757\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1758\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1759\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1760\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1761\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langchain/1-langchain_intro/.venv/lib/python3.12/site-packages/groq/_base_client.py:1576\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1573\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1575\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1576\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1578\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `nemotron-super-49b` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}",
      "During task with name 'model' and id '18122ffc-54a5-ec81-07fe-e0a52538897f'"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = await coordinator.ainvoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"I'm from London and I'd like a wedding in Paris for 100 guests, jazz-genre\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd822a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5ea57",
   "metadata": {},
   "source": [
    "link to trace: https://smith.langchain.com/public/7b5fe668-d3e3-4af4-b513-a8cacc0c9e84/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
