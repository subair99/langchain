{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71000bda-07a6-4fc4-9375-3ca9d3505942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='b024f377-5df0-405b-afd6-c4b5e5f6b32a'),\n",
      "              AIMessage(content='', additional_kwargs={'reasoning_content': \"We need to answer about langchain-mcp-adapters library. Likely a library related to LangChain? Let's search web.\", 'tool_calls': [{'id': 'fc_0751696a-612c-478b-908b-b740417fd188', 'function': {'arguments': '{\"query\":\"langchain-mcp-adapters library\"}', 'name': 'search_web'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 266, 'total_tokens': 326, 'completion_time': 0.12666639, 'completion_tokens_details': {'reasoning_tokens': 27}, 'prompt_time': 0.011611662, 'prompt_tokens_details': None, 'queue_time': 0.005009331, 'total_time': 0.138278052}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_7ee00986fe', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c52d1-dc11-7633-bebd-5405b53ccff0-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'fc_0751696a-612c-478b-908b-b740417fd188', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 266, 'output_tokens': 60, 'total_tokens': 326, 'output_token_details': {'reasoning': 27}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"response_time\": 0.76,\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP)\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library. import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"http\\\\\", // HTTP-based remote server transport:  \\\\\"http\\\\\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.87706697,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-tools/\",\\n      \"title\": \"langchain-mcp-tools\",\\n      \"content\": \"LangChain\\'s official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You might want to consider\",\\n      \"score\": 0.8755427,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.86981434,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.8593928,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"Tools adapter for converting MCP tools to LangChain tools. This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool\",\\n      \"score\": 0.8319231,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"request_id\": \"5e3561fe-0202-4302-94d2-e08674fa4cf2\"\\n}', 'id': 'lc_4b2fa17c-08ea-4193-a1e2-b3b7cf8050bf'}], name='search_web', id='a5db446d-5b1e-4966-a21e-f74bb583ed72', tool_call_id='fc_0751696a-612c-478b-908b-b740417fd188', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'response_time': 0.76, 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP)', 'content': '[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library. import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { ChatAnthropic } from \"@langchain/anthropic\"; import { ChatAnthropic } from \"@langchain/anthropic\";import { createAgent } from \"langchain\"; import { createAgent } from  \"langchain\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \"stdio\", // Local subprocess communication transport:  \"stdio\", // Local subprocess communication command: \"node\", command:  \"node\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\"/path/to/math_server.js\"], args: [\"/path/to/math_server.js\"], }, }, weather: { weather: { transport: \"http\", // HTTP-based remote server transport:  \"http\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \"http://localhost:8000/mcp\", url: \"http://localhost:8000/mcp\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.', 'score': 0.87706697, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-tools/', 'title': 'langchain-mcp-tools', 'content': \"LangChain's official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You might want to consider\", 'score': 0.8755427, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.86981434, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.8593928, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'Tools adapter for converting MCP tools to LangChain tools. This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool', 'score': 0.8319231, 'raw_content': None}], 'request_id': '5e3561fe-0202-4302-94d2-e08674fa4cf2'}}}),\n",
      "              AIMessage(content='**langchain‑mcp‑adapters** is a small, officially‑maintained library that bridges **Model\\u202fContext\\u202fProtocol (MCP)** tool servers with **LangChain** (and, by extension, LangGraph).  \\n\\n---\\n\\n## What problem does it solve?\\n\\n* **MCP** is an open protocol that lets a server expose “tools” (functions, APIs, local subprocesses, etc.) to LLMs in a standard way.  \\n* LangChain agents expect tools to be instances of `BaseTool` (Python) or `Tool` (JS/TS).  \\n* Writing a custom wrapper for every MCP server quickly becomes tedious.\\n\\n`langchain‑mcp‑adapters` automates that conversion:\\n\\n| MCP side | LangChain side |\\n|----------|----------------|\\n| `tool` defined in an MCP server (JSON‑RPC over HTTP, WebSocket, stdio, etc.) | `BaseTool` / `Tool` object that can be passed to `AgentExecutor`, `createAgent`, `Graph`, … |\\n| Multiple MCP servers | One client (`MultiServerMCPClient`) that aggregates all tools and presents them as a single list. |\\n| OAuth‑2 / API‑key protected MCP servers | Built‑in `authProvider` hook that injects the required headers. |\\n\\n---\\n\\n## Key concepts & classes  \\n\\n| Class / function | Language | What it does |\\n|------------------|----------|--------------|\\n| **`MultiServerMCPClient`** | Python & TypeScript | Takes a dictionary of server configs, connects to each server, fetches the tool manifests, and returns a list of LangChain‑compatible tools (`client.get_tools()`). |\\n| **`MCPToolAdapter`** (internal) | Python & TypeScript | Turns a single MCP tool description (name, description, input schema, transport, etc.) into a `BaseTool`/`Tool`. Handles the call‑routing (HTTP, stdio, websockets) and output parsing. |\\n| **`authProvider`** (optional) | TS only (JS has similar pattern) | Callable that receives a request and returns auth headers – useful for servers that require OAuth2, API keys, etc. |\\n| **`defaultToolTimeout`**, **`outputHandling`**, **`throwOnLoadError`** | Both | Convenience configuration knobs that control time‑outs, how tool output is interpreted, and whether a failure to load a tool aborts the whole client. |\\n\\n---\\n\\n## Installation  \\n\\n```bash\\n# Python\\npip install langchain-mcp-adapters   # pulls the Python package\\n\\n# JavaScript / TypeScript\\nnpm i @langchain/mcp-adapters   # or yarn add @langchain/mcp-adapters\\n```\\n\\nBoth packages depend only on the core `langchain` (or `langgraph`) libraries, so they work in any environment where LangChain does.\\n\\n---\\n\\n## Quick start – Python  \\n\\n```python\\nfrom langchain_mcp_adapters import MultiServerMCPClient\\nfrom langchain.agents import initialize_agent, AgentType\\nfrom langchain.llms import OpenAI\\n\\n# 1️⃣ Define the MCP servers you want to talk to\\nservers = {\\n    \"math\": {\\n        \"transport\": \"stdio\",\\n        \"command\": \"node\",\\n        \"args\": [\"/absolute/path/to/math_server.js\"],\\n    },\\n    \"weather\": {\\n        \"transport\": \"http\",\\n        \"url\": \"http://localhost:8000/mcp\",\\n    },\\n}\\n\\n# 2️⃣ Build the client and fetch LangChain tools\\nclient = MultiServerMCPClient(servers)\\ntools = client.get_tools()          # ← list[BaseTool]\\n\\n# 3️⃣ Plug the tools into any LangChain agent\\nllm = OpenAI(model=\"gpt-4o-mini\")\\nagent = initialize_agent(\\n    tools,\\n    llm,\\n    agent=AgentType.OPENAI_FUNCTIONS,   # or any other agent type that supports tools\\n    verbose=True,\\n)\\n\\n# 4️⃣ Use the agent\\nresponse = agent.run(\"What is the square root of 144 and does it rain in Paris today?\")\\nprint(response)\\n```\\n\\n*The `MultiServerMCPClient` automatically prefixes tool names with the server name (`math.sqrt`, `weather.get_current`) unless you set `prefix_tool_names=False`.*\\n\\n---\\n\\n## Quick start – TypeScript  \\n\\n```ts\\nimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";\\nimport { ChatAnthropic } from \"@langchain/anthropic\";\\nimport { createAgent } from \"langchain\";\\n\\n// 1️⃣ Server configuration\\nconst client = new MultiServerMCPClient({\\n  math: {\\n    math: {\\n      transport: \"stdio\",\\n      command: \"node\",\\n      args: [\"/path/to/math_server.js\"],\\n    },\\n  },\\n  weather: {\\n    weather: {\\n      transport: \"http\",\\n      url: \"http://localhost:8000/mcp\",\\n    },\\n  },\\n});\\n\\n// 2️⃣ Load tools (returns `Tool[]`)\\nconst tools = await client.getTools();\\n\\n// 3️⃣ Build an agent that can call those tools\\nconst llm = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" });\\nconst agent = await createAgent({\\n  llm,\\n  tools,\\n  // any LangGraph/Agent config you like\\n});\\n\\n// 4️⃣ Run a query\\nconst result = await agent.invoke({\\n  input: \"Calculate 7 * 8 and tell me if it will be sunny in Tokyo tomorrow.\",\\n});\\nconsole.log(result);\\n```\\n\\n---\\n\\n## Main features (at a glance)\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Multi‑server aggregation** | Pull tools from dozens of MCP servers with a single client call. |\\n| **Transport‑agnostic** | Supports `http`, `websocket`, `stdio` (local subprocess), and any future transport that follows the MCP JSON‑RPC spec. |\\n| **Automatic schema conversion** | MCP tool input schemas (JSON‑Schema) become LangChain `pydantic` models (Python) or `zod` schemas (TS), so the LLM gets proper type hints. |\\n| **Error handling knobs** | `throwOnLoadError`, `defaultToolTimeout`, and per‑tool `outputHandling` let you decide how strict the integration should be. |\\n| **Auth integration** | `authProvider` lets you inject OAuth2 tokens, API keys, or custom headers without writing wrapper code. |\\n| **LangGraph ready** | The returned `Tool` objects are fully compatible with LangGraph’s `ToolNode`, enabling graph‑based agents to call MCP tools out‑of‑the‑box. |\\n| **Zero‑runtime dependencies** | Apart from LangChain itself, the adapters only need `httpx` (Python) or `node-fetch` (TS) for HTTP transport. |\\n\\n---\\n\\n## When to use it\\n\\n* **You already have MCP tool servers** (e.g., the open‑source “math”, “weather”, “image‑generation” servers) and want to plug them into a LangChain or LangGraph agent without hand‑crafting wrappers.  \\n* **You need to combine tools from multiple domains** (e.g., a math server + a knowledge‑base server) in a single agent.  \\n* **You want future‑proofing** – as new MCP servers appear, you can just add them to the config and the same agent code continues to work.  \\n\\nIf you don’t need MCP at all (your tools are already native LangChain `Tool`s), you can skip the adapters and use LangChain directly.\\n\\n---\\n\\n## Where to read more\\n\\n* **Official docs** – <https://docs.langchain.com/oss/javascript/langchain/mcp> (JS) and <https://reference.langchain.com/python/langchain_mcp_adapters/> (Python).  \\n* **Changelog announcement** – <https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph>.  \\n* **GitHub source** – <https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters> (JS) and the analogous Python package under the `langchain` monorepo.  \\n\\n---\\n\\n### TL;DR\\n\\n`langchain‑mcp‑adapters` is the glue that lets LangChain/LangGraph agents **discover, load, and call** tools exposed via the **Model Context Protocol**. It handles transport, authentication, schema conversion, and multi‑server aggregation, so you can focus on building agents rather than writing boilerplate adapters.', additional_kwargs={'reasoning_content': \"We have search results. Need to answer about the library. Provide overview: purpose, features, usage, installation, key classes (MultiServerMCPClient, MCPToolAdapter), integration with LangChain and LangGraph, support for multiple servers, authentication, tool loading, examples. Could also reference docs. Provide concise answer.\\n\\nLet's craft answer.\"}, response_metadata={'token_usage': {'completion_tokens': 1862, 'prompt_tokens': 1528, 'total_tokens': 3390, 'completion_time': 3.961850891, 'completion_tokens_details': {'reasoning_tokens': 70}, 'prompt_time': 0.081352584, 'prompt_tokens_details': None, 'queue_time': 0.005096951, 'total_time': 4.043203475}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_dddd02f37b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c52d1-ee48-71c3-8565-caf73d08afe8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1528, 'output_tokens': 1862, 'total_tokens': 3390, 'output_token_details': {'reasoning': 70}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "585c9044-4dbe-4954-9e78-d300c294b350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**langchain‑mcp‑adapters** is a small, officially‑maintained library that bridges **Model Context Protocol (MCP)** tool servers with **LangChain** (and, by extension, LangGraph).  \n",
      "\n",
      "---\n",
      "\n",
      "## What problem does it solve?\n",
      "\n",
      "* **MCP** is an open protocol that lets a server expose “tools” (functions, APIs, local subprocesses, etc.) to LLMs in a standard way.  \n",
      "* LangChain agents expect tools to be instances of `BaseTool` (Python) or `Tool` (JS/TS).  \n",
      "* Writing a custom wrapper for every MCP server quickly becomes tedious.\n",
      "\n",
      "`langchain‑mcp‑adapters` automates that conversion:\n",
      "\n",
      "| MCP side | LangChain side |\n",
      "|----------|----------------|\n",
      "| `tool` defined in an MCP server (JSON‑RPC over HTTP, WebSocket, stdio, etc.) | `BaseTool` / `Tool` object that can be passed to `AgentExecutor`, `createAgent`, `Graph`, … |\n",
      "| Multiple MCP servers | One client (`MultiServerMCPClient`) that aggregates all tools and presents them as a single list. |\n",
      "| OAuth‑2 / API‑key protected MCP servers | Built‑in `authProvider` hook that injects the required headers. |\n",
      "\n",
      "---\n",
      "\n",
      "## Key concepts & classes  \n",
      "\n",
      "| Class / function | Language | What it does |\n",
      "|------------------|----------|--------------|\n",
      "| **`MultiServerMCPClient`** | Python & TypeScript | Takes a dictionary of server configs, connects to each server, fetches the tool manifests, and returns a list of LangChain‑compatible tools (`client.get_tools()`). |\n",
      "| **`MCPToolAdapter`** (internal) | Python & TypeScript | Turns a single MCP tool description (name, description, input schema, transport, etc.) into a `BaseTool`/`Tool`. Handles the call‑routing (HTTP, stdio, websockets) and output parsing. |\n",
      "| **`authProvider`** (optional) | TS only (JS has similar pattern) | Callable that receives a request and returns auth headers – useful for servers that require OAuth2, API keys, etc. |\n",
      "| **`defaultToolTimeout`**, **`outputHandling`**, **`throwOnLoadError`** | Both | Convenience configuration knobs that control time‑outs, how tool output is interpreted, and whether a failure to load a tool aborts the whole client. |\n",
      "\n",
      "---\n",
      "\n",
      "## Installation  \n",
      "\n",
      "```bash\n",
      "# Python\n",
      "pip install langchain-mcp-adapters   # pulls the Python package\n",
      "\n",
      "# JavaScript / TypeScript\n",
      "npm i @langchain/mcp-adapters   # or yarn add @langchain/mcp-adapters\n",
      "```\n",
      "\n",
      "Both packages depend only on the core `langchain` (or `langgraph`) libraries, so they work in any environment where LangChain does.\n",
      "\n",
      "---\n",
      "\n",
      "## Quick start – Python  \n",
      "\n",
      "```python\n",
      "from langchain_mcp_adapters import MultiServerMCPClient\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# 1️⃣ Define the MCP servers you want to talk to\n",
      "servers = {\n",
      "    \"math\": {\n",
      "        \"transport\": \"stdio\",\n",
      "        \"command\": \"node\",\n",
      "        \"args\": [\"/absolute/path/to/math_server.js\"],\n",
      "    },\n",
      "    \"weather\": {\n",
      "        \"transport\": \"http\",\n",
      "        \"url\": \"http://localhost:8000/mcp\",\n",
      "    },\n",
      "}\n",
      "\n",
      "# 2️⃣ Build the client and fetch LangChain tools\n",
      "client = MultiServerMCPClient(servers)\n",
      "tools = client.get_tools()          # ← list[BaseTool]\n",
      "\n",
      "# 3️⃣ Plug the tools into any LangChain agent\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "agent = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=AgentType.OPENAI_FUNCTIONS,   # or any other agent type that supports tools\n",
      "    verbose=True,\n",
      ")\n",
      "\n",
      "# 4️⃣ Use the agent\n",
      "response = agent.run(\"What is the square root of 144 and does it rain in Paris today?\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "*The `MultiServerMCPClient` automatically prefixes tool names with the server name (`math.sqrt`, `weather.get_current`) unless you set `prefix_tool_names=False`.*\n",
      "\n",
      "---\n",
      "\n",
      "## Quick start – TypeScript  \n",
      "\n",
      "```ts\n",
      "import { MultiServerMCPClient } from \"@langchain/mcp-adapters\";\n",
      "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
      "import { createAgent } from \"langchain\";\n",
      "\n",
      "// 1️⃣ Server configuration\n",
      "const client = new MultiServerMCPClient({\n",
      "  math: {\n",
      "    math: {\n",
      "      transport: \"stdio\",\n",
      "      command: \"node\",\n",
      "      args: [\"/path/to/math_server.js\"],\n",
      "    },\n",
      "  },\n",
      "  weather: {\n",
      "    weather: {\n",
      "      transport: \"http\",\n",
      "      url: \"http://localhost:8000/mcp\",\n",
      "    },\n",
      "  },\n",
      "});\n",
      "\n",
      "// 2️⃣ Load tools (returns `Tool[]`)\n",
      "const tools = await client.getTools();\n",
      "\n",
      "// 3️⃣ Build an agent that can call those tools\n",
      "const llm = new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" });\n",
      "const agent = await createAgent({\n",
      "  llm,\n",
      "  tools,\n",
      "  // any LangGraph/Agent config you like\n",
      "});\n",
      "\n",
      "// 4️⃣ Run a query\n",
      "const result = await agent.invoke({\n",
      "  input: \"Calculate 7 * 8 and tell me if it will be sunny in Tokyo tomorrow.\",\n",
      "});\n",
      "console.log(result);\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Main features (at a glance)\n",
      "\n",
      "| Feature | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Multi‑server aggregation** | Pull tools from dozens of MCP servers with a single client call. |\n",
      "| **Transport‑agnostic** | Supports `http`, `websocket`, `stdio` (local subprocess), and any future transport that follows the MCP JSON‑RPC spec. |\n",
      "| **Automatic schema conversion** | MCP tool input schemas (JSON‑Schema) become LangChain `pydantic` models (Python) or `zod` schemas (TS), so the LLM gets proper type hints. |\n",
      "| **Error handling knobs** | `throwOnLoadError`, `defaultToolTimeout`, and per‑tool `outputHandling` let you decide how strict the integration should be. |\n",
      "| **Auth integration** | `authProvider` lets you inject OAuth2 tokens, API keys, or custom headers without writing wrapper code. |\n",
      "| **LangGraph ready** | The returned `Tool` objects are fully compatible with LangGraph’s `ToolNode`, enabling graph‑based agents to call MCP tools out‑of‑the‑box. |\n",
      "| **Zero‑runtime dependencies** | Apart from LangChain itself, the adapters only need `httpx` (Python) or `node-fetch` (TS) for HTTP transport. |\n",
      "\n",
      "---\n",
      "\n",
      "## When to use it\n",
      "\n",
      "* **You already have MCP tool servers** (e.g., the open‑source “math”, “weather”, “image‑generation” servers) and want to plug them into a LangChain or LangGraph agent without hand‑crafting wrappers.  \n",
      "* **You need to combine tools from multiple domains** (e.g., a math server + a knowledge‑base server) in a single agent.  \n",
      "* **You want future‑proofing** – as new MCP servers appear, you can just add them to the config and the same agent code continues to work.  \n",
      "\n",
      "If you don’t need MCP at all (your tools are already native LangChain `Tool`s), you can skip the adapters and use LangChain directly.\n",
      "\n",
      "---\n",
      "\n",
      "## Where to read more\n",
      "\n",
      "* **Official docs** – <https://docs.langchain.com/oss/javascript/langchain/mcp> (JS) and <https://reference.langchain.com/python/langchain_mcp_adapters/> (Python).  \n",
      "* **Changelog announcement** – <https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph>.  \n",
      "* **GitHub source** – <https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters> (JS) and the analogous Python package under the `langchain` monorepo.  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "`langchain‑mcp‑adapters` is the glue that lets LangChain/LangGraph agents **discover, load, and call** tools exposed via the **Model Context Protocol**. It handles transport, authentication, schema conversion, and multi‑server aggregation, so you can focus on building agents rather than writing boilerplate adapters.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='8b7cc6ee-3ffe-4cf8-9e0f-0c6c79b282f9'),\n",
      "              AIMessage(content=\"I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\", additional_kwargs={'reasoning_content': 'User asks \"What time is it?\" Not related to LangChain etc. According to instructions, we must respond with \"I\\'m sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"'}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 426, 'total_tokens': 496, 'completion_time': 0.147075274, 'completion_tokens_details': {'reasoning_tokens': 43}, 'prompt_time': 0.020547354, 'prompt_tokens_details': None, 'queue_time': 0.004762654, 'total_time': 0.167622628}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8634b911ff', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c52d2-57f1-7750-bfe3-74b5bc23cdc3-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 426, 'output_tokens': 70, 'total_tokens': 496, 'output_token_details': {'reasoning': 43}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a337475-4877-414a-ae80-1c9017f9dda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
