{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfd8398-66d8-4a0e-bf68-544d541fd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"gpt-oss:20b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='f5a8280f-789d-48ee-8e35-a43b382d01dd'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2026-02-25T11:16:28.65070441Z', 'done': True, 'done_reason': 'stop', 'total_duration': 236552118256, 'load_duration': 134058557292, 'prompt_eval_count': 268, 'prompt_eval_duration': 81574114669, 'eval_count': 70, 'eval_duration': 19553588546, 'logprobs': None, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--019c9480-2d5c-7ac3-bbb2-da41c90e1671-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'f04e190a-2470-411a-9b79-875c2393d528', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 268, 'output_tokens': 70, 'total_tokens': 338}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.99999475,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.9999658,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca\",\\n      \"title\": \"The Complete Guide to langchain-mcp-adapters - Medium\",\\n      \"content\": \"The `langchain-mcp-adapters` library represents a significant advancement in AI agent development, serving as a bridge between LangChain & LangGraph and Anthropic\\'s Model Context Protocol (MCP). This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. Anthropic introduced MCP as an open standard to help developers build secure, two-way connections between their data sources and AI-powered tools. The adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers, allowing agents to pull from multiple MCP servers at once for more powerful applications. from langchain_mcp_adapters.tools import load_mcp_tools. When an MCP server returns content with multiple parts like text and images, the adapter converts them to LangChain’s standard content blocks:. DEBUG=\\'@langchain/mcp-adapters:tools\\'. By bridging LangChain\\'s powerful agent framework with MCP\\'s standardized tool ecosystem, it enables developers to build sophisticated, multi-tool applications without the burden of custom integrations.\",\\n      \"score\": 0.999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.9999443,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \\\\\"user_id\\\\\": user_id}  args ={**request.args, \\\\\"user_id\\\\\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\\\\\"gpt-4.1\\\\\", tools, context_schema=Context) agent = create_agent(\\\\\"gpt-4.1\\\\\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, context={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"}  context ={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"})).\",\\n      \"score\": 0.999943,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.77,\\n  \"request_id\": \"00d8a473-802e-44d0-8897-4569701ecba4\"\\n}', 'id': 'lc_82e84e5a-148e-44f2-81f8-eeed36f34ad8'}], name='search_web', id='6edb54d1-f96d-4bc8-b2c1-91a6f0bdff8f', tool_call_id='f04e190a-2470-411a-9b79-875c2393d528', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.99999475, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.9999658, 'raw_content': None}, {'url': 'https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca', 'title': 'The Complete Guide to langchain-mcp-adapters - Medium', 'content': \"The `langchain-mcp-adapters` library represents a significant advancement in AI agent development, serving as a bridge between LangChain & LangGraph and Anthropic's Model Context Protocol (MCP). This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. Anthropic introduced MCP as an open standard to help developers build secure, two-way connections between their data sources and AI-powered tools. The adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers, allowing agents to pull from multiple MCP servers at once for more powerful applications. from langchain_mcp_adapters.tools import load_mcp_tools. When an MCP server returns content with multiple parts like text and images, the adapter converts them to LangChain’s standard content blocks:. DEBUG='@langchain/mcp-adapters:tools'. By bridging LangChain's powerful agent framework with MCP's standardized tool ecosystem, it enables developers to build sophisticated, multi-tool applications without the burden of custom integrations.\", 'score': 0.999956, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.9999443, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \"\"\"Inject user credentials into MCP tool calls.\"\"\" \"\"\"Inject user credentials into MCP tool calls.\"\"\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \"user_id\": user_id}  args ={**request.args, \"user_id\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\"gpt-4.1\", tools, context_schema=Context) agent = create_agent(\"gpt-4.1\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}  context ={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"})).', 'score': 0.999943, 'raw_content': None}], 'response_time': 0.77, 'request_id': '00d8a473-802e-44d0-8897-4569701ecba4'}}}),\n",
      "              AIMessage(content='**langchain‑mcp‑adapters** is a lightweight, open‑source package that bridges Anthropic’s **Model Context Protocol (MCP)** with the LangChain and LangGraph ecosystems.  \\nIt lets you treat MCP‑based tools as first‑class LangChain tools, so you can build agents that call any MCP server without writing custom adapters.\\n\\n---\\n\\n## Why use it?\\n\\n| Problem | How MCP adapters solve it |\\n|---------|---------------------------|\\n| **Manual tool conversion** – Every MCP tool needs a custom LangChain wrapper. | The adapter automatically converts an MCP tool definition into a LangChain‑compatible `Tool` object. |\\n| **Multiple MCP servers** – You want to pull tools from several servers (e.g., math, weather, command). | `MultiServerMCPClient` lets you connect to any number of MCP servers and load all their tools in one go. |\\n| **Secure servers** – Some MCP servers require OAuth2 or custom headers. | The client accepts an `authProvider` or custom headers, so you don’t have to manage the HTTP layer yourself. |\\n| **Tool‑output handling** – MCP can return multi‑part responses (text, images, etc.). | The adapter normalises those into LangChain’s `ContentBlock` format. |\\n\\n---\\n\\n## Core components\\n\\n| Component | Purpose |\\n|-----------|---------|\\n| `MultiServerMCPClient` | Connects to one or more MCP servers, loads tools, and handles authentication. |\\n| `load_mcp_tools` | Convenience helper that returns a list of LangChain `Tool` objects from a client. |\\n| `MCPToolCallRequest` | Interceptor interface for injecting context (e.g., user ID, API key) into tool calls. |\\n| `MCPToolCallResponse` | Normalises MCP responses into LangChain’s `ContentBlock` list. |\\n\\n---\\n\\n## Typical usage\\n\\n```python\\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\\nfrom langchain_mcp_adapters.tools import load_mcp_tools\\nfrom langchain.agents import create_agent\\nfrom langchain.chat_models import ChatOpenAI\\n\\n# 1. Create a client that talks to two MCP servers\\nclient = MultiServerMCPClient(\\n    \"math\",\\n    \"weather\",\\n    transport=\"http\",\\n    url=\"http://localhost:8000/mcp\",\\n    authProvider=None,          # or a custom OAuth2 provider\\n)\\n\\n# 2. Load all tools from those servers\\ntools = await client.get_tools()          # or: tools = await load_mcp_tools(client)\\n\\n# 3. Build an agent that can call any of those tools\\nmodel = ChatOpenAI(model=\"gpt-4.1\")\\nagent = create_agent(model, tools)\\n\\n# 4. Invoke the agent\\nresponse = await agent.ainvoke(\"What’s 3 + 5 times 12?\")\\n\\nprint(response)\\n```\\n\\n**With LangGraph**\\n\\n```python\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.prebuilt import ToolNode, tools_condition\\nfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\"openai:gpt-4.1\")\\nclient = MultiServerMCPClient(\"math\", \"weather\", transport=\"http\", url=\"http://localhost:8000/mcp\")\\ntools = await client.get_tools()\\n\\nbuilder = StateGraph()\\nbuilder.add_node(\"call_model\", lambda state: model.bind_tools(tools).invoke(state[\"messages\"]))\\nbuilder.add_node(\"tools\", ToolNode(tools))\\nbuilder.add_edge(\"start\", \"call_model\")\\nbuilder.add_conditional_edges(\"call_model\", tools_condition)\\nbuilder.add_edge(\"tools\", \"call_model\")\\n\\ngraph = builder.compile()\\nresult = await graph.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"What\\'s the weather in NYC?\"}]})\\n```\\n\\n---\\n\\n## Key features\\n\\n| Feature | Detail |\\n|---------|--------|\\n| **Zero‑code conversion** | MCP tools become LangChain tools automatically. |\\n| **Multi‑server support** | Pull tools from any number of MCP servers in a single client. |\\n| **Authentication helpers** | OAuth2, API keys, or custom headers are handled by the client. |\\n| **Output normalisation** | Text, images, and other content types are converted to LangChain `ContentBlock`s. |\\n| **Interceptors** | Inject user context or modify requests/responses before they hit the model. |\\n| **Timeouts** | Per‑tool or global default timeouts can be configured. |\\n\\n---\\n\\n## Where to find it\\n\\n* **GitHub repo** – <https://github.com/langchain-ai/langchain-mcp-adapters>\\n* **NPM package** – `@langchain/mcp-adapters` (for JavaScript/TypeScript projects)\\n* **Docs** – <https://docs.langchain.com/oss/python/langchain/mcp>\\n\\n---\\n\\n### TL;DR\\n\\n`langchain-mcp-adapters` lets you treat any MCP‑based tool as a native LangChain tool, so you can build agents that call hundreds of existing MCP servers with minimal boilerplate. It handles connection, authentication, tool conversion, and output normalisation, making it a plug‑and‑play bridge between Anthropic’s MCP ecosystem and LangChain/LangGraph.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2026-02-25T11:26:14.710693582Z', 'done': True, 'done_reason': 'stop', 'total_duration': 580447471467, 'load_duration': 452408003, 'prompt_eval_count': 2307, 'prompt_eval_duration': 245379698119, 'eval_count': 1151, 'eval_duration': 333582462605, 'logprobs': None, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--019c9483-d738-71c0-9cbd-a016161e13c9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 2307, 'output_tokens': 1151, 'total_tokens': 3458})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcb0b1-b170-43be-a300-7330a724a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='f8f09108-256e-43a8-8047-a2b4d247cf81'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2026-01-03T20:22:15.514302358Z', 'done': True, 'done_reason': 'stop', 'total_duration': 48337020616, 'load_duration': 205435868, 'prompt_eval_count': 363, 'prompt_eval_duration': 42056111853, 'eval_count': 21, 'eval_duration': 6008899074, 'logprobs': None, 'model_name': 'llama3.1:8b', 'model_provider': 'ollama'}, id='lc_run--019b8585-a5f4-7522-bb02-213ac884ab66-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': '17b731cc-2154-4f43-aa1a-133e720594ae', 'type': 'tool_call'}], usage_metadata={'input_tokens': 363, 'output_tokens': 21, 'total_tokens': 384}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-03T15:22:16-05:00\",\\n  \"day_of_week\": \"Saturday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_50eb24d9-049e-4612-8c1a-c09f30b1e144'}], name='get_current_time', id='ae11e536-faba-4eea-8ec5-cc97157abdac', tool_call_id='17b731cc-2154-4f43-aa1a-133e720594ae'),\n",
      "              AIMessage(content='The current time is 3:22 PM.', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2026-01-03T20:22:30.878088425Z', 'done': True, 'done_reason': 'stop', 'total_duration': 14567265940, 'load_duration': 140573877, 'prompt_eval_count': 142, 'prompt_eval_duration': 11439410895, 'eval_count': 11, 'eval_duration': 2947084779, 'logprobs': None, 'model_name': 'llama3.1:8b', 'model_provider': 'ollama'}, id='lc_run--019b8586-65f5-7ae3-b32a-31a33ed96b1e-0', usage_metadata={'input_tokens': 142, 'output_tokens': 11, 'total_tokens': 153})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
