from dotenv import load_dotenv

load_dotenv()





from langchain_ollama import ChatOllama
from langchain.agents import create_agent

model = ChatOllama(
    model="llama3.1:8b", 
    temperature=0
)

agent = create_agent(
    model=model
)


from langchain.messages import HumanMessage

question = HumanMessage(content="Hello my name is Seán and my favourite colour is green")

response = agent.invoke(
    {"messages": [question]} 
)


from pprint import pprint

pprint(response)


question = HumanMessage(content="What's my favourite colour?")

response = agent.invoke(
    {"messages": [question]} 
)

pprint(response)





from langgraph.checkpoint.memory import InMemorySaver  


agent = create_agent(
    model=model,
    checkpointer=InMemorySaver(),  
)


from langchain.messages import HumanMessage

question = HumanMessage(content="Hello my name is Seán and my favourite colour is green")
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
    {"messages": [question]},
    config,  
)


pprint(response)


question = HumanMessage(content="What's my favourite colour?")

response = agent.invoke(
    {"messages": [question]},
    config,  
)

pprint(response)



