from dotenv import load_dotenv

load_dotenv()





from langchain.chat_models import init_chat_model

model = init_chat_model(model="gpt-5-nano")


response = model.invoke("What's the capital of the Moon?")

response


print(response.content)


from pprint import pprint

pprint(response.response_metadata)





model = init_chat_model(
    model="gpt-5-nano",
    # Kwargs passed to the model:
    temperature=1.0
)

response = model.invoke("What's the capital of the Moon?")
print(response.content)








model = init_chat_model(model="claude-sonnet-4-5")

response = model.invoke("What's the capital of the Moon?")
print(response.content)


from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")

response = model.invoke("What's the capital of the Moon?")
print(response.content)





from langchain.agents import create_agent

agent = create_agent(model=model)


agent = create_agent(model="claude-sonnet-4-5")


agent = create_agent("gpt-5-nano")


from langchain.messages import HumanMessage

response = agent.invoke(
    {"messages": [HumanMessage(content="What's the capital of the Moon?")]}
)


from pprint import pprint

pprint(response)


print(response['messages'][-1].content)


from langchain.messages import AIMessage

response = agent.invoke(
    {"messages": [HumanMessage(content="What's the capital of the Moon?"),
    AIMessage(content="The capital of the Moon is Luna City."),
    HumanMessage(content="Interesting, tell me more about Luna City")]}
)

pprint(response)





for token, metadata in agent.stream(
    {"messages": [HumanMessage(content="Tell me all about Luna City, the capital of the Moon")]},
    stream_mode="messages"
):

    # token is a message chunk with token content
    # metadata contains which node produced the token
    
    if token.content:  # Check if there's actual content
        print(token.content, end="", flush=True)  # Print token



