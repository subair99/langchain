





from dotenv import load_dotenv

load_dotenv()


from langchain_ollama import ChatOllama
from langchain.agents import create_agent

model = ChatOllama(model="gpt-oss:20b", temperature=0.8)

agent = create_agent(
    model=model,
    system_prompt="You are a science fiction writer, create a capital city at the users request.",
)


from langchain.messages import HumanMessage

question = HumanMessage(content=[
    {"type": "text", "text": "What is the capital of The Moon?"}
])

response = agent.invoke(
    {"messages": [question]}
)

print(response['messages'][-1].content)





from ipywidgets import FileUpload
from IPython.display import display

uploader = FileUpload(accept='.jpg', multiple=False)
display(uploader)


print(uploader.value)


import base64

# Get the first (and only) uploaded file dict
uploaded_file = uploader.value[0]

# This is a memoryview
content_mv = uploaded_file["content"]

# Convert memoryview -> bytes
img_bytes = bytes(content_mv)  # or content_mv.tobytes()

# Now base64 encode
img_b64 = base64.b64encode(img_bytes).decode("utf-8")


model = ChatOllama(model="qwen3-vl:8b", temperature=0.8)

agent = create_agent(
    model=model,
    system_prompt="You are a science fiction writer, create a capital city at the users request.",
)


multimodal_question = HumanMessage(content=[
    {"type": "text", "text": "Tell me about this capital"},
    {"type": "image", "base64": img_b64, "mime_type": "image/.jpg"}
])

response = agent.invoke(
    {"messages": [multimodal_question]}
)

print(response['messages'][-1].content)





from ipywidgets import FileUpload
from IPython.display import display

uploader = FileUpload(accept='.mp3', multiple=False)
display(uploader)


print(uploader.value)


import base64
import io
from faster_whisper import WhisperModel
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

# 1. Setup the Transcription & LLM
stt_model = WhisperModel("base", device="cpu", compute_type="int8")
model = ChatOllama(model="gpt-oss:20b", temperature=0.8)

# 2. Extract and Encode Audio from Uploader
if uploader.value:
    uploaded_file = uploader.value[0]
    content_mv = uploaded_file["content"]
    
    # Convert to base64 (Your requested step)
    aud_bytes = bytes(content_mv)
    aud_b64 = base64.b64encode(aud_bytes).decode("utf-8")
    
    # 3. Transcribe Audio
    print("Transcribing voice input...")
    audio_data = base64.b64decode(aud_b64)
    audio_file = io.BytesIO(audio_data)
    
    segments, _ = stt_model.transcribe(audio_file, beam_size=5)
    transcribed_text = " ".join([segment.text for segment in segments])
    print(f"User said: {transcribed_text}")
    
    # 4. Agent Response
    response = model.invoke([
        HumanMessage(content=f"The user said: '{transcribed_text}'. Provide a helpful response.")
    ])
    
    print("\n--- AI Response ---")
    print(response.content)
else:
    print("Please upload an audio file (.wav) first.")



