from dotenv import load_dotenv

load_dotenv()





from langchain.chat_models import init_chat_model

model = init_chat_model(model="gpt-oss:20b",  model_provider="ollama")


response = model.invoke("What's the capital of the Moon?")

response


print(response.content)


from pprint import pprint

pprint(response.response_metadata)





model = init_chat_model(model="gpt-oss:20b",  model_provider="ollama", temperature=1.0)

response = model.invoke("What's the capital of the Moon?")
print(response.content)








model = init_chat_model(model="qwen3:14b",  model_provider="ollama")

response = model.invoke("What's the capital of the Moon?")
print(response.content)





model = init_chat_model(model="gpt-oss:20b",  model_provider="ollama")


from langchain.agents import create_agent

agent = create_agent(model=model)


from langchain.messages import HumanMessage

response = agent.invoke(
    {"messages": [HumanMessage(content="What's the capital of the Moon?")]}
)


from pprint import pprint

pprint(response)


print(response['messages'][-1].content)


from langchain.messages import AIMessage

response = agent.invoke(
    {"messages": [HumanMessage(content="What's the capital of the Moon?"),
    AIMessage(content="The capital of the Moon is Luna City."),
    HumanMessage(content="Interesting, tell me more about Luna City")]}
)

pprint(response)








for token, metadata in agent.stream(
    {"messages": [HumanMessage(content="Tell me all about Luna City, the capital of the Moon")]},
    stream_mode="messages"
):

    # token is a message chunk with token content
    # metadata contains which node produced the token
    
    if token.content:  # Check if there's actual content
        print(token.content, end="", flush=True)  # Print token



